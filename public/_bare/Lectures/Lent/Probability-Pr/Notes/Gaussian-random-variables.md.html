<p>
    <div class="markdown-embed embed-definition">
        <div class="markdown-embed-title">
            Definition (Gaussian random variables)
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Definition-Gaussian-random-variables.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                A random variable <span class="inline-math">$X$</span> in <span class="inline-math">$\R$</span> is Gaussian
                if <span class="inline-math">$X=\sigma Z+\mu$</span> for some <span class="inline-math">$\mu\in\R, \sigma\in[0,\infty)$</span> and <span class="inline-math">$Z\sim N(0,1)$</span>, i.e. the probability distribution function is
                <span class="display-math">$$ \begin{align*}
                    f_Z(z) = \frac{1}{\sqrt{2\pi}} e^{-z^2/2}.
                \end{align*} $$</span>
                Then we write <span class="inline-math">$X\sim N(\mu, \sigma^2)$</span>.
            </p>
            <p>
                For <span class="inline-math">$n\geq 2$</span>, a random variable <span class="inline-math">$X$</span> in <span class="inline-math">$\R^n$</span> is Gaussian if <span class="inline-math">$u^T X \equiv \sum_{i=1}^{n} u_i X_i$</span> is Gaussian for all <span class="inline-math">$u\in\R^n$</span>.
            </p>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed embed-result">
        <div class="markdown-embed-title">
            Proposition (affine functions of Gaussian are Gaussian)
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Proposition-affine-functions-of-Gaussian-are-Gaussian.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                For a Guassian random variable <span class="inline-math">$X$</span> in <span class="inline-math">$\R^n$</span>, for any <span class="inline-math">$m\times n$</span> matrix <span class="inline-math">$a$</span> and vector <span class="inline-math">$b\in\R^m$</span>,
                the random variable <span class="inline-math">$Y = aX + b$</span> is also Gaussian. (<a href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Proof-affine-functions-of-Gaussian-are-Gaussian.html">Proof</a>)
            </p>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed">
        <div class="markdown-embed-title">
            Moment generating function of Gaussian random variable
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Moment-generating-function-of-Gaussian-random-variable.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                For a Gaussian random variable <span class="inline-math">$X$</span> in <span class="inline-math">$\R^n$</span>, suppose it has mean <span class="inline-math">$\mu = \expect(X)$</span> and variance <span class="inline-math">$V = \op{var}(X) = \left(\op{cov}(X_i, X_j)\right)_{i,j=1}^n$</span>.
            </p>
            <p>
                For all <span class="inline-math">$u\in\R^n$</span>, the random variable <span class="inline-math">$u^T X \sim N(u^T \mu, u^T V u)$</span>. (<a href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Derivation-mean-and-variance-of-projection-of-multivariate-random-variable.html">Derivation</a>)
                Note that <span class="inline-math">$u^T V u \geq 0$</span> so <span class="inline-math">$V$</span> is non-negative definite.
            </p>
            <p>
                Therefore the  generating function is
                <span class="display-math">$$ \begin{align*}
                    M_X(\lambda) = \expect(e^{\lambda^T X}) = e^{\lambda^T \mu + \frac{1}{2}\lambda^T V\lambda}.
                \end{align*} $$</span>
                <a href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Theorem-uniqueness-of-moment-generating-function.html">Hence</a> the distribution of <span class="inline-math">$X$</span> is uniquely determined by <span class="inline-math">$\mu$</span> and <span class="inline-math">$V$</span>.
                So we write <span class="inline-math">$X\sim N(\mu, V)$</span>.
            </p>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed">
        <div class="markdown-embed-title">
            Constructing Guassian random variables
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Constructing-Guassian-random-variables.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                To construct <span class="inline-math">$Z\sim N(0, I_n)$</span> where <span class="inline-math">$I_n$</span> is the <span class="inline-math">$n\times n$</span> identity matrix,
                take <span class="inline-math">$Z_1, \ldots, Z_n$</span> independent <span class="inline-math">$N(0,1)$</span> random varibales and set <span class="inline-math">$Z = (Z_1, \ldots, Z_n)$</span>.
                (<a href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Proof-constructing-Gaussian-random-variables.html">Proof</a>)
            </p>
            <p>
                Generally to construct <span class="inline-math">$X\sim N(\mu, V)$</span>, we define <span class="inline-math">$X=\sigma Z + \mu$</span> where <span class="inline-math">$\sigma = \sqrt{V}$</span>.
                <span class="inline-math">$X$</span> is then Gaussian, <span class="inline-math">$\expect(X) = \mu$</span>, and
                <span class="display-math">$$\op{var}(X) = \expect((\sigma Z)(\sigma Z)^T) = \sigma\op{var}(Z)\sigma^T = \sigma I_n \sigma^T = V.$$</span>
                Hence <span class="inline-math">$X \sim N(\mu, V)$</span>.
            </p>
            <p>
                (What is <span class="inline-math">$\sqrt{V}$</span>: since <span class="inline-math">$V$</span> is symmetric, <span class="inline-math">$V$</span> is diagonalisable by an orthogonal matrix <span class="inline-math">$U$</span>.
                Also since <span class="inline-math">$V$</span> is non-negative definite, <span class="inline-math">$V$</span> has eigenvalues <span class="inline-math">$\lambda_1, \ldots, \lambda_n \geq 0$</span>.
                <span class="display-math">$$ \begin{align*}
                    V &amp;= U\Lambda U^T, &amp; \quad \Lambda &amp;= \op{diag}(\lambda_1, \ldots, \lambda_n). \\
                    \text{Set } \sigma &amp;= U\sqrt{\Lambda} U^T, &amp;\sqrt{\Lambda} &amp;= \op{diag}(\sqrt{\lambda_1}, \ldots, \sqrt{\lambda_n})
                \end{align*} $$</span>
                so we have <span class="inline-math">$\sigma\sigma^T = V$</span>.)
            </p>
            <p>
                For positive definite variance <span class="inline-math">$V$</span> (equivalently <span class="inline-math">$V$</span> is invertible), the density function of <span class="inline-math">$X\sim N(\mu, V)$</span> is
                <span class="display-math">$$ \begin{align*}
                    f_X(x) = \frac{\exp\left(-\frac{1}{2} (x-\mu)^T V\inv (x-\mu)\right)}{\sqrt{(2\pi)^n \det V}}.
                \end{align*} $$</span>
                (<a href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Derivation-density-function-of-Gaussian-random-variables-for-positive-definite-variance.html">Derivation</a>)
            </p>
            <p>
                If <span class="inline-math">$V$</span> is not invertible, then <span class="inline-math">$U$</span> can be choosen so that <span class="inline-math">$UX \sim N(U\mu, UVU^T)$</span> with
                <span class="display-math">$$ \begin{align*}
                    UVU^T = \op{diag}(\lambda_1, \lambda_2, \ldots, \lambda_k, 0, \ldots, 0) \equiv \pmat{V_0 &amp; 0 \\ 0 &amp; 0}.
                \end{align*} $$</span>
                where <span class="inline-math">$V_0$</span> is positive definite.
                Then we can construct
                <span class="display-math">$$ \begin{align*}
                    X = \mu + U^T \pmat{Y\\0}, \quad Y = \sqrt{V_0}\pmat{z_1 \\ z_2 \\ \vdots \\ z_k}
                \end{align*} $$</span>
                and <span class="inline-math">$Y$</span> has density
                <span class="display-math">$$ \begin{align*}
                    f_Y(y) = \frac{\exp\left(-\frac{1}{2} y^T V_0\inv y\right)}{\sqrt{(2\pi)^n\det V_0}}
                \end{align*} $$</span>
            </p>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed">
        <div class="markdown-embed-title">
            Bivariate normal
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Bivariate-normal.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                Let <span class="inline-math">$X$</span> be a non-degenerate normal random variable in <span class="inline-math">$\R^2$</span>, i.e. <span class="inline-math">$V=\var(X)$</span> is invertible.
                <span class="inline-math">$X$</span> is uniquely determined by each <span class="inline-math">$(\mu_1, \mu_2, \sigma_1^2, \sigma_2^2, \rho) \in \R^2\times (0,\infty)^2 \times (-1, 1)$</span>, where <span class="inline-math">$\mu_i = \expect(X_i), \sigma_i^2 = \expect(X_i), \rho = \op{corr}(X_1,X_2)$</span>.
                (<a href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Proof-bivariate-normal-determined-by-5-parameters.html">Proof</a>)
            </p>
            <p>
                If <span class="inline-math">$\op{corr}(X_1, X_2)=0$</span>, then <span class="inline-math">$X_1, X_2$</span> are independent. (<a href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Proof-Zero-correlation-implies-independence-for-bivariate-normal.html">Proof</a>)
                For a general <span class="inline-math">$\rho$</span>, we can write
                <span class="display-math">$$ \begin{align*}
                    X_2 = aX_1 + Y
                \end{align*} $$</span>
                where <span class="inline-math">$Y$</span> is normal and independent of <span class="inline-math">$X_1$</span>, and <span class="inline-math">$a = \rho\sigma_2/\sigma_1$</span>. (<a href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Derivation-bivariate-normal-as-sums-of-independent-Gaussians.html">Derivation</a>)
            </p>
            
        </div>
    </div>
</p>

<p>
    <div class="markdown-embed embed-definition">
        <div class="markdown-embed-title">
            Definition (Gaussian random variables)
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Definition-Gaussian-random-variables.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                A random variable <span class="inline-math">$X$</span> in <span class="inline-math">$\R$</span> is Gaussian
                if <span class="inline-math">$X=\sigma Z+\mu$</span> for some <span class="inline-math">$\mu\in\R, \sigma\in[0,\infty)$</span> and <span class="inline-math">$Z\sim N(0,1)$</span>, i.e. the probability distribution function is
                <span class="display-math">$$ \begin{align*}
                    f_Z(z) = \frac{1}{\sqrt{2\pi}} e^{-z^2/2}.
                \end{align*} $$</span>
                Then we write <span class="inline-math">$X\sim N(\mu, \sigma^2)$</span>.
            </p>
            <p>
                For <span class="inline-math">$n\geq 2$</span>, a random variable <span class="inline-math">$\v X$</span> in <span class="inline-math">$\R^n$</span> is Gaussian if <span class="inline-math">$\v u^T \v X \equiv \sum_{i=1}^{n} u_i X_i$</span> is Gaussian for all <span class="inline-math">$\v u\in\R^n$</span>.
            </p>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed embed-result">
        <div class="markdown-embed-title">
            Proposition (affine functions of Gaussian are Gaussian)
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Proposition-affine-functions-of-Gaussian-are-Gaussian.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                For a Guassian random variable <span class="inline-math">$\v X$</span> in <span class="inline-math">$\R^n$</span>, for any <span class="inline-math">$m\times n$</span> matrix <span class="inline-math">$\matr a$</span> and vector <span class="inline-math">$\v b\in\R^m$</span>,
                the random variable <span class="inline-math">$\v Y = \matr a\v X + \v b$</span> is also Gaussian. (<a href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Proof-affine-functions-of-Gaussian-are-Gaussian.html">Proof</a>)
            </p>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed">
        <div class="markdown-embed-title">
            Moment generating function of Gaussian random variable
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Moment-generating-function-of-Gaussian-random-variable.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                For a Gaussian random variable <span class="inline-math">$\v X$</span> in <span class="inline-math">$\R^n$</span>,
                suppose it has mean <span class="inline-math">$\v \mu = \expect(\v X)$</span> and variance <span class="inline-math">$\matr V = \op{var}(\v X) = \left(\op{cov}(X_i, X_j)\right)_{i,j=1}^n$</span>.
            </p>
            <p>
                For all <span class="inline-math">$\v u\in\R^n$</span>, the random variable <span class="inline-math">$\v u^T \v X \sim N(\v u^T \v \mu, \v u^T V \v u)$</span>. (<a href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Derivation-mean-and-variance-of-projection-of-multivariate-random-variable.html">Derivation</a>)
                Note that <span class="inline-math">$\v u^T \matr V \v u \geq 0$</span> so <span class="inline-math">$\matr V$</span> is non-negative definite.
            </p>
            <p>
                Therefore the  generating function is
                <span class="display-math">$$ \begin{align*}
                    M_{\v X}(\v\lambda) = \expect(e^{\v\lambda^T \v X}) = e^{\v \lambda^T \v \mu + \frac{1}{2}\v \lambda^T \matr V\v \lambda}.
                \end{align*} $$</span>
                <a href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Theorem-uniqueness-of-moment-generating-function.html">Hence</a> the distribution of <span class="inline-math">$\v X$</span> is uniquely determined by <span class="inline-math">$\v \mu$</span> and <span class="inline-math">$\matr V$</span>.
                So we write <span class="inline-math">$\v X\sim N(\v \mu, \matr V)$</span>.
            </p>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed">
        <div class="markdown-embed-title">
            Constructing Guassian random variables
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Constructing-Guassian-random-variables.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                To construct <span class="inline-math">$\v Z\sim N(0, I_n)$</span> where <span class="inline-math">$I_n$</span> is the <span class="inline-math">$n\times n$</span> identity matrix,
                take <span class="inline-math">$Z_1, \ldots, Z_n$</span> independent <span class="inline-math">$N(0,1)$</span> random varibales and set <span class="inline-math">$\v Z = (Z_1, \ldots, Z_n)$</span>.
                (<a href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Proof-constructing-Gaussian-random-variables.html">Proof</a>)
            </p>
            <p>
                Generally to construct <span class="inline-math">$\v X\sim N(\v \mu, \matr V)$</span>, we define <span class="inline-math">$\v X=\matr \sigma \v Z + \v \mu$</span> where <span class="inline-math">$\matr \sigma = \sqrt{\matr V}$</span>.
                <span class="inline-math">$\v X$</span> is then Gaussian, <span class="inline-math">$\expect(\v X) = \v \mu$</span>, and
                <span class="display-math">$$\op{var}(\v X) = \expect((\matr \sigma \v Z)(\matr \sigma \v Z)^T) = \matr \sigma\op{var}(\v Z)\matr \sigma^T = \matr \sigma I_n \matr \sigma^T = \matr V.$$</span>
                Hence <span class="inline-math">$\v X \sim N(\v\mu,\matr V)$</span>.
            </p>
            <p>
                (What is <span class="inline-math">$\sqrt{\matr V}$</span>: since <span class="inline-math">$\matr V$</span> is symmetric, <span class="inline-math">$\matr V$</span> is diagonalisable by an orthogonal matrix <span class="inline-math">$\matr U$</span>.
                Also since <span class="inline-math">$\matr V$</span> is non-negative definite, <span class="inline-math">$\matr V$</span> has eigenvalues <span class="inline-math">$\lambda_1, \ldots, \lambda_n \geq 0$</span>.
                <span class="display-math">$$ \begin{align*}
                    \matr V &amp;= \matr U\matr \Lambda \matr U^T, &amp; \quad \matr \Lambda &amp;= \op{diag}(\lambda_1, \ldots, \lambda_n). \\
                    \text{Set } \matr \sigma &amp;= \matr U\sqrt{\matr \Lambda} \matr U^T, &amp;\sqrt{\matr \Lambda} &amp;= \op{diag}(\sqrt{\lambda_1}, \ldots, \sqrt{\lambda_n})
                \end{align*} $$</span>
                so we have <span class="inline-math">$\matr \sigma\matr \sigma^T = \matr V$</span>.)
            </p>
            <p>
                For positive definite variance <span class="inline-math">$\matr V$</span> (equivalently <span class="inline-math">$\matr V$</span> is invertible), the density function of <span class="inline-math">$\v X\sim N(\v \mu, \matr V)$</span> is
                <span class="display-math">$$ \begin{align*}
                    f_\v X(\v x) = \frac{\exp\left(-\frac{1}{2} (\v x-\v \mu)^T \matr V\inv (\v x-\v \mu)\right)}{\sqrt{(2\pi)^n \det \matr V}}.
                \end{align*} $$</span>
                (<a href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Derivation-density-function-of-Gaussian-random-variables-for-positive-definite-variance.html">Derivation</a>)
            </p>
            <p>
                If <span class="inline-math">$\matr V$</span> is not invertible, then <span class="inline-math">$\matr U$</span> can be choosen so that <span class="inline-math">$\matr U\v X \sim N(\matr U\v\mu, \matr U\matr V\matr U^T)$</span> with
                <span class="display-math">$$ \begin{align*}
                    \matr U\matr V\matr U^T = \op{diag}(\lambda_1, \lambda_2, \ldots, \lambda_k, 0, \ldots, 0) \equiv \pmat{\matr V_0 &amp; \matr 0 \\ \matr 0 &amp; \matr 0}.
                \end{align*} $$</span>
                where <span class="inline-math">$\matr V_0$</span> is positive definite.
                Then we can construct
                <span class="display-math">$$ \begin{align*}
                    \v X = \v \mu + \matr U^T \pmat{\v Y\\\v 0}, \quad \v Y = \sqrt{\matr V_0}\pmat{z_1 \\ z_2 \\ \vdots \\ z_k}
                \end{align*} $$</span>
                and <span class="inline-math">$\v Y$</span> has density
                <span class="display-math">$$ \begin{align*}
                    f_\v Y(\v y) = \frac{\exp\left(-\frac{1}{2} \v y^T \matr V_0\inv \v y\right)}{\sqrt{(2\pi)^n\det \matr V_0}}
                \end{align*} $$</span>
            </p>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed">
        <div class="markdown-embed-title">
            Bivariate normal
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Bivariate-normal.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                Let <span class="inline-math">$\v X$</span> be a non-degenerate normal random variable in <span class="inline-math">$\R^2$</span>, i.e. <span class="inline-math">$\matr V=\var(\v X)$</span> is invertible.
                <span class="inline-math">$\v X$</span> is uniquely determined by each <span class="inline-math">$(\mu_1, \mu_2, \sigma_1^2, \sigma_2^2, \rho) \in \R^2\times (0,\infty)^2 \times (-1, 1)$</span>, where <span class="inline-math">$\mu_i = \expect(X_i), \sigma_i^2 = \expect(X_i), \rho = \op{corr}(X_1,X_2)$</span>.
                (<a href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Proof-bivariate-normal-determined-by-5-parameters.html">Proof</a>)
            </p>
            <p>
                If <span class="inline-math">$\op{corr}(X_1, X_2)=0$</span>, then <span class="inline-math">$X_1, X_2$</span> are independent. (<a href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Proof-Zero-correlation-implies-independence-for-bivariate-normal.html">Proof</a>)
                For a general <span class="inline-math">$\rho$</span>, we can write
                <span class="display-math">$$ \begin{align*}
                    X_2 = aX_1 + Y
                \end{align*} $$</span>
                where <span class="inline-math">$Y$</span> is normal and independent of <span class="inline-math">$X_1$</span>, and <span class="inline-math">$a = \rho\sigma_2/\sigma_1$</span>. (<a href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Derivation-bivariate-normal-as-sums-of-independent-Gaussians.html">Derivation</a>)
            </p>
            
        </div>
    </div>
</p>

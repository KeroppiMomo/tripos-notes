<p>
    To construct <span class="inline-math">$\v Z\sim N(0, I_n)$</span> where <span class="inline-math">$I_n$</span> is the <span class="inline-math">$n\times n$</span> identity matrix,
    take <span class="inline-math">$Z_1, \ldots, Z_n$</span> independent <span class="inline-math">$N(0,1)$</span> random varibales and set <span class="inline-math">$\v Z = (Z_1, \ldots, Z_n)$</span>.
    (<a href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Proof-constructing-Gaussian-random-variables.html">Proof</a>)
</p>
<p>
    Generally to construct <span class="inline-math">$\v X\sim N(\v \mu, \matr V)$</span>, we define <span class="inline-math">$\v X=\matr \sigma \v Z + \v \mu$</span> where <span class="inline-math">$\matr \sigma = \sqrt{\matr V}$</span>.
    <span class="inline-math">$\v X$</span> is then Gaussian, <span class="inline-math">$\expect(\v X) = \v \mu$</span>, and
    <span class="display-math">$$\op{var}(\v X) = \expect((\matr \sigma \v Z)(\matr \sigma \v Z)^T) = \matr \sigma\op{var}(\v Z)\matr \sigma^T = \matr \sigma I_n \matr \sigma^T = \matr V.$$</span>
    Hence <span class="inline-math">$\v X \sim N(\v\mu,\matr V)$</span>.
</p>
<p>
    (What is <span class="inline-math">$\sqrt{\matr V}$</span>: since <span class="inline-math">$\matr V$</span> is symmetric, <span class="inline-math">$\matr V$</span> is diagonalisable by an orthogonal matrix <span class="inline-math">$\matr U$</span>.
    Also since <span class="inline-math">$\matr V$</span> is non-negative definite, <span class="inline-math">$\matr V$</span> has eigenvalues <span class="inline-math">$\lambda_1, \ldots, \lambda_n \geq 0$</span>.
    <span class="display-math">$$ \begin{align*}
        \matr V &amp;= \matr U\matr \Lambda \matr U^T, &amp; \quad \matr \Lambda &amp;= \op{diag}(\lambda_1, \ldots, \lambda_n). \\
        \text{Set } \matr \sigma &amp;= \matr U\sqrt{\matr \Lambda} \matr U^T, &amp;\sqrt{\matr \Lambda} &amp;= \op{diag}(\sqrt{\lambda_1}, \ldots, \sqrt{\lambda_n})
    \end{align*} $$</span>
    so we have <span class="inline-math">$\matr \sigma\matr \sigma^T = \matr V$</span>.)
</p>
<p>
    For positive definite variance <span class="inline-math">$\matr V$</span> (equivalently <span class="inline-math">$\matr V$</span> is invertible), the density function of <span class="inline-math">$\v X\sim N(\v \mu, \matr V)$</span> is
    <span class="display-math">$$ \begin{align*}
        f_\v X(\v x) = \frac{\exp\left(-\frac{1}{2} (\v x-\v \mu)^T \matr V\inv (\v x-\v \mu)\right)}{\sqrt{(2\pi)^n \det \matr V}}.
    \end{align*} $$</span>
    (<a href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Derivation-density-function-of-Gaussian-random-variables-for-positive-definite-variance.html">Derivation</a>)
</p>
<p>
    If <span class="inline-math">$\matr V$</span> is not invertible, then <span class="inline-math">$\matr U$</span> can be choosen so that <span class="inline-math">$\matr U\v X \sim N(\matr U\v\mu, \matr U\matr V\matr U^T)$</span> with
    <span class="display-math">$$ \begin{align*}
        \matr U\matr V\matr U^T = \op{diag}(\lambda_1, \lambda_2, \ldots, \lambda_k, 0, \ldots, 0) \equiv \pmat{\matr V_0 &amp; \matr 0 \\ \matr 0 &amp; \matr 0}.
    \end{align*} $$</span>
    where <span class="inline-math">$\matr V_0$</span> is positive definite.
    Then we can construct
    <span class="display-math">$$ \begin{align*}
        \v X = \v \mu + \matr U^T \pmat{\v Y\\\v 0}, \quad \v Y = \sqrt{\matr V_0}\pmat{z_1 \\ z_2 \\ \vdots \\ z_k}
    \end{align*} $$</span>
    and <span class="inline-math">$\v Y$</span> has density
    <span class="display-math">$$ \begin{align*}
        f_\v Y(\v y) = \frac{\exp\left(-\frac{1}{2} \v y^T \matr V_0\inv \v y\right)}{\sqrt{(2\pi)^n\det \matr V_0}}
    \end{align*} $$</span>
</p>

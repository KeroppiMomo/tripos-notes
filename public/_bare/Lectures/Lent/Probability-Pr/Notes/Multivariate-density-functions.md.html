<p>
    <div class="markdown-embed embed-definition">
        <div class="markdown-embed-title">
            Definition (joint probability density function)
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Definition-joint-probability-density-function.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                We say a random variable <span class="inline-math">$X = (X_1, \ldots, X_n)$</span> in <span class="inline-math">$\R^n$</span> has probability density function <span class="inline-math">$f_X$</span>
                if for all <span class="inline-math">$x_1, \ldots, x_n\in\R$</span>,
                <span class="display-math">$$ \begin{align*}
                    F_X(x_1, \ldots, x_n) \equiv \prob(X_1\leq x_1, \ldots, X_n\leq x_n) = \int_{-\infty}^{x_1} \ldots \int_{-\infty}^{x_n} f_X(x) \, dx
                \end{align*} $$</span>
                Equivalently for all Borel set <span class="inline-math">$B$</span>,
                <span class="display-math">$$ \begin{align*}
                    \prob(X\in B) = \int_{B} f_X(x) \, dx.
                \end{align*} $$</span>
                Also equivalently for all non-negative Borel function <span class="inline-math">$g$</span>,
                <span class="display-math">$$ \begin{align*}
                    \expect(g(X)) = \int_{\R^n} g(x) f_X(x) \, dx.
                \end{align*} $$</span>
            </p>
            
        </div>
    </div>
    <div class="markdown-embed embed-definition">
        <div class="markdown-embed-title">
            Definition (marginal density function)
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Definition-marginal-density-function.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                Suppose <span class="inline-math">$X=(X_1, X_2, \ldots, X_n)$</span> is a random variable in <span class="inline-math">$\R^n$</span> having density function <span class="inline-math">$f_X$</span>.
                Then each <span class="inline-math">$X_i$</span> has a density function <span class="inline-math">$f_{X_i}$</span> called <em>marginal density function</em> given by
                <span class="display-math">$$ \begin{align*}
                    f_{X_i}(x_i) = \int_{\R^{n-1}} f_X(x_1, x_2, \ldots, x_n) \, \prod_{j\neq i} dx_j.
                \end{align*} $$</span>
            </p>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed embed-result">
        <div class="markdown-embed-title">
            Theorem (independence iff joint pdf factorises)
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Theorem-independence-iff-joint-pdf-factorises.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                Let <span class="inline-math">$X=(X_1, \ldots, X_n)$</span> be a random variable in <span class="inline-math">$\R^n$</span>,
                and density functions <span class="inline-math">$f_1, \ldots, f_n$</span>.
            </p>
            <p>
                <span class="inline-math">$X_1, \ldots X_n$</span> are independent and have marginal density functions <span class="inline-math">$f_1, \ldots, f_n$</span>
                iff <span class="inline-math">$X$</span> has density function <span class="inline-math">$f_X$</span> given by
                <span class="display-math">$$ \begin{align*}
                    f_X(x) = \prod_{i=1}^{n} f_i(x_i).
                \end{align*} $$</span>
            </p>
            <p>
                <a href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Proof-independence-iff-joint-pdf-factorises.html">Proof</a>
            </p>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed embed-result">
        <div class="markdown-embed-title">
            Theorem (convolution of densities for sum of independent random variables)
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Theorem-convolution-of-densities-for-sum-of-independent-random-variables.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                Let <span class="inline-math">$X,Y$</span> be independent random variables in <span class="inline-math">$\R$</span> with density functions <span class="inline-math">$f_X, f_Y$</span>.
            </p>
            <p>
                Then <span class="inline-math">$Z=X+Y$</span> has a density function given by the convolution
                <span class="display-math">$$ \begin{align*}
                    f_Z(x) = f_X * f_Y(x) \equiv \int_{\R} f(x-y) g(y) \, dy.
                \end{align*} $$</span>
            </p>
            <p>
                <a href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Proof-convolution-of-densities-for-sum-of-independent-random-variables.html">Proof</a>
            </p>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed embed-result">
        <div class="markdown-embed-title">
            Theorem (transformation of multi-dimensional random variable)
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Lent/Probability-Pr/Notes/Theorem-transformation-of-multi-dimensional-random-variable.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                Let <span class="inline-math">$X$</span> be a random variable in <span class="inline-math">$\R^n$</span> taking values and having a density function <span class="inline-math">$f_X$</span> in an open set <span class="inline-math">$D$</span>.
                Let <span class="inline-math">$\phi$</span> maps <span class="inline-math">$D$</span> bijectively to <span class="inline-math">$\phi(D) \subseteq \R^n$</span> and
                suppose <span class="inline-math">$\phi$</span> has a continuous derivative (Jacobian matrix <span class="inline-math">$\phi&#x27;$</span>) with <span class="inline-math">$\phi&#x27;(x) \neq 0$</span>.
            </p>
            <p>
                Set <span class="inline-math">$y=\phi(x)$</span> and define a new random variable <span class="inline-math">$Y=\phi(X)$</span>.
                Then <span class="inline-math">$Y$</span> has a density function in <span class="inline-math">$\phi(D)$</span> given by
                <span class="display-math">$$ \begin{align*}
                    f_Y(y) = f_X(x)\,|J|
                \end{align*} $$</span>
                where <span class="inline-math">$J$</span> is the Jacobian
                <span class="display-math">$$ \begin{align*}
                    J = \det\left(\pdfd{x_i}{y_j}\right) = \left(\det\left(\pdfd{y_i}{x_j}\right)\right)\inv.
                \end{align*} $$</span>
            </p>
            
        </div>
    </div>
</p>

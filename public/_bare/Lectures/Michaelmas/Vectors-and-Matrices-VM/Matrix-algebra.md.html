<p>
    <div class="markdown-embed">
        <div class="markdown-embed-title">
            Matrix addition
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Matrix-addition.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                Consider two linear maps <span class="inline-math">$\alpha,\beta: \R^n\to \R^m$</span>.
                Define the sum of linear maps as <span class="inline-math">$(\alpha+\beta)(\vec x) = \alpha(\vec x) + \beta(\vec x)$</span>.
                The associated matrices satisfy
                <span class="display-math">$$ (A+B)_{ij} x_j = A_{ij} x_j + B_{ij} x_j $$</span>
                so we define matrix addition as just adding it entry by entry:
                <span class="display-math">$$ (A+B)_{ij} = A_{ij} + B_{ij}. $$</span>
            </p>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed">
        <div class="markdown-embed-title">
            Matrix scalar multiplication
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Matrix-scalar-multiplication.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                We define the scalar multiplication of a linear map as
                <span class="display-math">$$ (\lambda\alpha)(\vec x) = \lambda\cdot\alpha(\vec x). $$</span>
                Therefore
                <span class="display-math">$$ (\lambda A)_{ij} = \lambda A_{ij}. $$</span>
            </p>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed">
        <div class="markdown-embed-title">
            Matrix multiplication
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Matrix-multiplication.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                Consider <span class="inline-math">$\alpha: \R^r \to \R^n$</span> and <span class="inline-math">$\beta: \R^n\to\R^m$</span>. <span class="inline-math">$A$</span> is the associated <span class="inline-math">$n\times r$</span> matrix and <span class="inline-math">$B$</span> is the associated <span class="inline-math">$m\times n$</span> matrix.
            </p>
            <p>
                We consider the composition map <span class="inline-math">$\beta\alpha: \R^r\to\R^m$</span>.
                <span class="display-math">$$\begin{align*}
                (BA)_{ij} x_j &amp;= \big(B(A\vec x)\big)_i \\
                &amp;= B_{ik}(A\vec x)_k \\
                &amp;= B_{ik}A_{kj}x_j
                \end{align*}$$</span>
                so we let
                <span class="display-math">$$ (BA)_{ij} = B_{ik}A_{kj}. $$</span>
            </p>
            <p>
                Properties:
            </p>
            <ul>
            <li>
                <p>
                    In general, <span class="inline-math">$AB\neq BA$</span>.
                </p>
                
            </li><li>
                <p>
                    Associativity: <span class="inline-math">$(AB)C = A(BC)$</span>. <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-associativity-of-matrix-multiplication.html">Proof</a>
                </p>
                
            </li>
            </ul>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed">
        <div class="markdown-embed-title">
            Matrix transpose
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Matrix-transpose.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                Let <span class="inline-math">$A$</span> be a <span class="inline-math">$m\times n$</span> matrix. The transpose of <span class="inline-math">$A$</span>, denoted as <span class="inline-math">$A^T$</span>, is a <span class="inline-math">$n\times m$</span> matrix defined by
                <span class="display-math">$$ (A^T)_{ij} = A_{ji}. $$</span>
            </p>
            <p>
                Properties:
            </p>
            <ul>
            <li>
                <p>
                    <span class="inline-math">$(A^T)^T = A$</span>.
                </p>
                
            </li><li>
                <p>
                    <span class="inline-math">$(AB)^T = B^T A^T$</span>. <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-transpose-of-matrix-product.html">Proof</a>
                </p>
                
            </li>
            </ul>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed">
        <div class="markdown-embed-title">
            Matrix inverse
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Matrix-inverse.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <ul>
            <li>
                <p>
                    <span class="inline-math">$B$</span> is the left inverse of <span class="inline-math">$A$</span> if <span class="inline-math">$BA = I$</span>.
                </p>
                
            </li><li>
                <p>
                    <span class="inline-math">$C$</span> is the right inverse of <span class="inline-math">$A$</span> if <span class="inline-math">$AC = I$</span>.
                </p>
                
            </li><li>
                <p>
                    If <span class="inline-math">$A$</span> is a square matrix, then <span class="inline-math">$A^{-1}\equiv B=C$</span> is the inverse of <span class="inline-math">$A$</span>. <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-left-inverse-equals-right-inverse-for-square-matrix.html">Proof</a> This means inverse <span class="inline-math">$A\inv$</span> satisfies
                    <span class="display-math">$$ A\inv A = AA \inv = I. $$</span>
                </p>
                
            </li><li>
                <p>
                    If <span class="inline-math">$A\inv$</span> exists, then <span class="inline-math">$A$</span> is called <em>invertible</em>.
                </p>
                
            </li><li>
                <p>
                    <span class="inline-math">$(AB)\inv = B\inv A\inv$</span>. <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-inverse-of-matrix-product.html">Proof</a>
                </p>
                
            </li>
            </ul>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed">
        <div class="markdown-embed-title">
            Hermitian conjugate
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Hermitian-conjugate.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                For a complex matrix <span class="inline-math">$A$</span>,
                <span class="display-math">$$ A^\dagger \equiv (A^T)^* \equiv (A^*)^T. $$</span>
            </p>
            <p>
                Properties:
            </p>
            <ul>
            <li>
                <p>
                    <span class="inline-math">$(AB)^\dagger = B^\dagger A^\dagger$</span>.
                </p>
                
            </li>
            </ul>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed">
        <div class="markdown-embed-title">
            Common type of matrices
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Common-type-of-matrices.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                <div class="markdown-embed embed-definition">
                    <div class="markdown-embed-title">
                        Definition (symmetric matrix)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-symmetric-matrix.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            A square matrix <span class="inline-math">$A$</span> is symmetric if <span class="display-math">$$ A^T = A. $$</span>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed embed-definition">
                    <div class="markdown-embed-title">
                        Definition (Hermitian matrix)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-Hermitian-matrix.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            A square matrix <span class="inline-math">$A$</span> is Hermitian if
                            <span class="display-math">$$ A^\dagger = A. $$</span>
                        </p>
                        <p>
                            The diagonal elements are necessarily real.
                        </p>
                        <p>
                            A real symmetric matrix is Hermitian.
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed embed-definition">
                    <div class="markdown-embed-title">
                        Definition (anti-symmetric matrix)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-anti-symmetric-matrix.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            A square matrix is anti-symmetric if
                            <span class="display-math">$$ A^T = -A. $$</span>
                            The diagonal elements are necessarily 0.
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed embed-definition">
                    <div class="markdown-embed-title">
                        Definition (skew-Hermitian matrix)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-skew-Hermitian-matrix.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            A square matrix is skew-Hermitian <span class="inline-math">$A$</span> if
                            <span class="display-math">$$ A^\dagger = -A. $$</span>
                        </p>
                        <p>
                            The diagonal elements are necessarily purely imaginary.
                        </p>
                        <p>
                            A real anti-symmetric matrix is a skew-Hermitian matrix.
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed embed-definition">
                    <div class="markdown-embed-title">
                        Definition (orthogonal and unitary matrix)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-orthogonal-and-unitary-matrix.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            A real matrix <span class="inline-math">$A$</span> is orthogonal if
                            <span class="display-math">$$ A^T = A\inv \quad\text{i.e. } AA^T = A^T A = I. $$</span>
                        </p>
                        <p>
                            A complex matrix <span class="inline-math">$U$</span> is unitary if
                            <span class="display-math">$$ U^\dagger = U\inv \quad\text{i.e. } UU^\dagger = U^\dagger U = I. $$</span>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed embed-definition">
                    <div class="markdown-embed-title">
                        Definition (normal matrix)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-normal-matrix.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            An <span class="inline-math">$n\times n$</span> matrix <span class="inline-math">$N$</span> is called <em>normal</em> if it commutes with its Hermitian conjugate
                            <span class="display-math">$$ NN^\dagger = N^\dagger N. $$</span>
                        </p>
                        <p>
                            Hermitian, symmetric, skew-Hermitian, anti-symmetric, orthogonal, unitary matrices are all normal.
                        </p>
                        
                    </div>
                </div>
            </p>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed">
        <div class="markdown-embed-title">
            Trace of a square matrix
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Trace-of-a-square-matrix.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                For a matrix <span class="inline-math">$A$</span>, the trace of <span class="inline-math">$A$</span>, denoted as <span class="inline-math">$\mrm{trace}(A)$</span>, is the sum of the diagonal elements of <span class="inline-math">$A$</span>,
                <span class="display-math">$$ \mrm{trace}(A) = A_{ii}. $$</span>
            </p>
            <p>
                Properties:
            </p>
            <ul>
            <li>
                <p>
                    <span class="inline-math">$\mrm{trace}(BC) = \mrm{trace}(CB)$</span>. <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-commutativity-of-trace.html">Proof</a>
                </p>
                
            </li>
            </ul>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed embed-definition">
        <div class="markdown-embed-title">
            Definition (isotropic matrix)
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-isotropic-matrix.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                A matrix <span class="inline-math">$A$</span> is isotropic if
                <span class="display-math">$$ A = \lambda I $$</span>
                for some scalar <span class="inline-math">$\lambda$</span>.
            </p>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed">
        <div class="markdown-embed-title">
            Decomposition of a square matrix
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Decomposition-of-a-square-matrix.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                Any <span class="inline-math">$n\times n$</span> matrix <span class="inline-math">$B$</span> can be written as a sum of symmetric and anti-symmetric parts,
                <span class="display-math">$$\begin{align*}
                B_{ij} &amp;= \frac12 (B_{ij} + B_{ji}) + \frac12 (B_{ij} - B_{ji}) \\
                B &amp;= \underbrace{\frac12 (B + B^T)}_{\text{symmetric } S} + \underbrace{\frac12 (B - B^T)}_{\text{anti-symmetric } A}
                \end{align*}$$</span>
            </p>
            <p>
                A symmetric matrix <span class="inline-math">$S$</span> can be decomposed an isotropic part and a traceless part,
                <span class="display-math">$$\begin{align*}
                S_{ij} &amp;= \frac1n \mrm{trace}(S)\, \delta_{ij} + \big(S_{ij} - \frac1n \mrm{trace}(S)\, \delta_{ij} \big) \\
                S &amp;= \underbrace{\frac1n \mrm{trace}(S)\, I}_\text{isotropic} + \underbrace{\big(S - \frac1n\mrm{trace}(S)\, I\big)}_{\text{traceless, since } \mrm{trace}(\frac1n I) = 1}
                \end{align*}$$</span>
            </p>
            <p>
                In <span class="inline-math">$\R^3$</span>, the anti-symmetric part <span class="inline-math">$A$</span> can be written in terms of a single vector <span class="inline-math">$\vec w$</span> as
                <span class="display-math">$$ A_{ijk} = \epsilon_{ijk}\omega_k = \begin{pmatrix}
                0 &amp; \omega_3 &amp; -\omega_2 \\
                -\omega_3 &amp; 0 &amp; \omega_1 \\
                \omega_2 &amp; -\omega_1 &amp; 0
                \end{pmatrix} $$</span>
                which is a general anti-symmetric matrix.
            </p>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed embed-result">
        <div class="markdown-embed-title">
            Proposition (rows and columns of orthogonal and unitary matrix are orthonormal)
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-rows-and-columns-of-orthogonal-and-unitary-matrix-are-orthonormal.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                For an orthogonal matrix <span class="inline-math">$A$</span>, the rows of <span class="inline-math">$A$</span> form an orthonormal set. Similarly the columns of <span class="inline-math">$A$</span> also form an orthonormal set.
                Same for a unitary matrix.
            </p>
            <p>
                <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-rows-and-columns-of-orthogonal-and-unitary-matrix-are-orthonormal.html">Proof</a>
            </p>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed">
        <div class="markdown-embed-title">
            Gram-Schmidt process
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Gram-Schmidt-process.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                Given a set <span class="inline-math">$\set{\v w_1, \v w_2, \ldots, \v w_r}$</span> of linearly independent vectors,
                we can use this process to form a orthonormal set <span class="inline-math">$\set{\v v_1, \v v_2, \ldots, \v w_r}$</span> that has the same span.
            </p>
            <p>
                Define the projection of <span class="inline-math">$\v w$</span> onto <span class="inline-math">$\v v$</span> by
                <span class="display-math">$$ \mathcal P_{\v v}(\v w) = \frac{\langle\v v|\v w\rangle}{\langle\v v|\v v\rangle}\v v.$$</span>
            </p>
            <ul>
            <li>
                <p>
                    Step 1: let <span class="inline-math">$\v v_1 = \v w_1 / \sqrt{\langle\v w_1|\v w_1\rangle}$</span>.
                </p>
                
            </li><li>
                <p>
                    Step <span class="inline-math">$k$</span>: let <span class="display-math">$$\v u_k = \v w_k - \sum_{i=1}^{k-1} \mathcal P_{\v v_i}(\v w_k), $$</span>
                    which gives an orthogonal vector. Then normalise it by
                    <span class="display-math">$$ \v v_k = \frac{\v u_k}{\sqrt{\langle\v u_k|\v u_k\rangle}}. $$</span>
                    (<a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Explanation-Gram-Schmidt-process.html">Explanation</a>)
                </p>
                
            </li>
            </ul>
            
        </div>
    </div>
</p>

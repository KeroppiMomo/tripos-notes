<p>
    <div class="markdown-embed">
        <div class="markdown-embed-title">
            Complex Numbers
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Complex-Numbers.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Basic properties of complex numbers
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Basic-properties-of-complex-numbers.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            <div class="markdown-embed embed-definition">
                                <div class="markdown-embed-title">
                                    Definition (complex number)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-complex-number.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        A complex number <span class="inline-math">$z\in\mathbb C$</span> is of the form
                                        <span class="display-math">$$ z = a+ib $$</span>
                                        where <span class="inline-math">$a,b\in\mathbb R$</span> and <span class="inline-math">$i=\sqrt{-1}$</span>.
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Algebraic manipulation
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Algebraic-manipulation.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <ul>
                                    <li>
                                        <p>
                                            <span class="inline-math">$z_1 \pm z_2$</span>;
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            <span class="inline-math">$z_1z_2$</span>;
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            modulus <span class="inline-math">$|z| = \sqrt{a^2 + b^2}$</span>;
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            conjugate <span class="inline-math">$\bar z = a - ib$</span>;
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            <span class="inline-math">$z\bar z = |z|^2$</span>;
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            <span class="inline-math">$z^{-1} = \bar z/|z|^2$</span>.
                                        </p>
                                        
                                    </li>
                                    </ul>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Proposition (triangle inequality for the complex numbers)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-triangle-inequality-for-the-complex-numbers.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        For all <span class="inline-math">$z_1,z_2 \in \mathbb C$</span>,
                                        <span class="display-math">$$ |z_1 + z_2| \leq |z_1| + |z_2|. $$</span>
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-triangle-inequality-for-complex-numbers.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                            (proof technique)
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Proposition (alt. triangle inequality)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-alt.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        For all <span class="inline-math">$z_1, z_2\in\mathbb C$</span>,
                                        <span class="display-math">$$ |z_1 - z_2| \geq \big||z_1| - |z_2|\big|. $$</span>
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-alt.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                            (proof technique)
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Polar representation
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Polar-representation.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        When <span class="inline-math">$z = r(\cos\theta + i\sin\theta)$</span>, we say <span class="inline-math">$r=|z|$</span> and <span class="inline-math">$\theta=\arg z$</span>, the argument of <span class="inline-math">$z$</span>. (Also <span class="inline-math">$z=re^{i\theta}$</span> by <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-Euler-s-formula.html">this</a>)
                                    </p>
                                    <p>
                                        Usually we restrict <span class="inline-math">$\theta$</span> to the principal value in the range <span class="inline-math">$(-\pi, \pi]$</span>.
                                    </p>
                                    <p>
                                        <div class="markdown-embed embed-result">
                                            <div class="markdown-embed-title">
                                                Proposition (multiplication in polar representation)
                                                <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-multiplication-in-polar-representation.html">open_in_new</a>
                                            </div>
                                            <div class="markdown-embed-content">
                                                <p>
                                                    For two complex numbers <span class="inline-math">$z_i = r_i(\cos\theta_i + i\sin\theta_i)$</span> for <span class="inline-math">$i=1,2$</span>,
                                                    <span class="display-math">$$ z_1z_2 = r_1r_2(\cos(\theta_1 + \theta_2) + i\sin(\theta_1 + \theta_2)). $$</span>
                                                </p>
                                                <p>
                                                    <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-multiplication-in-polar-representation.html">Proof</a>
                                                </p>
                                                
                                            </div>
                                        </div>
                                        i.e. moduli multiply, arguments add.
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Complex exponential function
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Complex-exponential-function.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            <div class="markdown-embed embed-definition">
                                <div class="markdown-embed-title">
                                    Definition (exponential function)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Lent/Analysis-I-An/Notes/Definition-exponential-function.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Define <span class="inline-math">$\exp: \mathbb C\to\mathbb C$</span> with
                                        <span class="display-math">$$ \exp(z) = \sum_{k=0}^\infty \frac{z^k}{k!} = 1 + z + \frac{1}{2!}z^2 + \frac{1}{3!}z^3 + \ldots. $$</span>
                                    </p>
                                    <p>
                                        This is well-defined as the series has a radius of convergence of <span class="inline-math">$\infty$</span>, since <span class="inline-math">$|a_{n+1}/a_n| = |k!/(k+1)!| = 1/(k+1) \to 0$</span>.
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            This series converges for all <span class="inline-math">$z\in\mathbb C$</span>. The proof will be given in analysis I.
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Proposition (adding exponents)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-adding-exponents.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        For all <span class="inline-math">$z_1, z_2\in\mathbb C$</span>,
                                        <span class="display-math">$$ \exp(z_1)\exp(z_2) = \exp(z_1+z_2). $$</span>
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-adding-exponents.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-definition">
                                <div class="markdown-embed-title">
                                    Definition (sin and cos)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-sin-and-cos.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        For any <span class="inline-math">$z\in\mathbb C$</span>,
                                        <span class="display-math">$$\begin{align*}
                                        \sin z &amp;= \sum_{k=0}^\infty \frac{(-1)^k z^{2k+1}}{(2k+1)!} \\
                                        \cos z &amp;= \sum_{k=0}^\infty \frac{(-1)^k z^{2k}}{(2k)!}
                                        \end{align*}$$</span>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Proposition (Euler's formula)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-Euler-s-formula.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        For any <span class="inline-math">$z\in\mathbb C$</span>,
                                        <span class="display-math">$$ e^{iz} = \cos z + i\sin z. $$</span>
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-Euler-s-formula.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Roots of unity
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Roots-of-unity.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Proposition (roots of unity)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-roots-of-unity.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        The solution to the equation
                                        <span class="display-math">$$ z^n = 1, $$</span>
                                        where <span class="inline-math">$n\in\mathbb N_{&gt;0}$</span> is a constant and <span class="inline-math">$z\in\mathbb C$</span>, is
                                        <span class="display-math">$$ z = \exp\Big(\frac{2\pi ik}{n}\Big),\quad k=0,1,2,\ldots,n-1. $$</span>
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-roots-of-unity.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Proposition (sum of roots of unity)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-sum-of-roots-of-unity.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Let <span class="inline-math">$w$</span> be the <span class="inline-math">$n$</span>-th <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-roots-of-unity.html">root of unity</a> <span class="inline-math">$\exp(2\pi i/n)$</span>. Then
                                        <span class="display-math">$$ 1+w+w^2+\ldots + w^{n-1} = 0. $$</span>
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-sum-of-roots-of-unity.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Complex logarithm and powers
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Complex-logarithm-and-powers.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            <div class="markdown-embed embed-definition">
                                <div class="markdown-embed-title">
                                    Definition (complex logarithm)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-complex-logarithm.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        The complex logarithm <span class="inline-math">$\log z$</span> where <span class="inline-math">$z\in\mathbb C$</span> is a solution to the equation <span class="inline-math">$e^w = z$</span>.
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            For any complex number <span class="inline-math">$z=re^{i\theta}\neq 0$</span>,
                            <span class="display-math">$$\begin{align*}
                            \log z &amp;= \log(re^{i\theta}) \\
                            &amp;= \log r + i\theta \\
                            &amp;= \log |z| + i\arg z + 2\pi ik,\quad k\in\mathbb Z
                            \end{align*}$$</span>
                            with the <em>principal value</em> defined for <span class="inline-math">$k=0$</span> and <span class="inline-math">$\arg z\in(-\pi,\pi]$</span>.
                        </p>
                        <p>
                            <div class="markdown-embed embed-definition">
                                <div class="markdown-embed-title">
                                    Definition (complex power)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-complex-power.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        For any <span class="inline-math">$z,\alpha\in\mathbb C$</span>, the complex power <span class="inline-math">$z^\alpha$</span> is defined as
                                        <span class="display-math">$$ z^\alpha = \exp(\alpha\log z). $$</span>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            Combining with the result above,
                            <span class="display-math">$$ z^\alpha = e^{\alpha\log|z|}e^{i\alpha\arg z} e^{2\pi i\alpha k},\quad k\in\mathbb Z. $$</span>
                            If <span class="inline-math">$\alpha\in\mathbb Q$</span> then there is only a finite number of values.
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Geometry in C
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Geometry-in-C.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Proposition (straight line in the complex plane)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-straight-line-in-the-complex-plane.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        For a line <span class="inline-math">$L$</span> in the complex plane that passes through <span class="inline-math">$z_0\in\mathbb C$</span> and parallel to <span class="inline-math">$w\in\mathbb C$</span>, a general point <span class="inline-math">$z\in L$</span> has equation
                                        <span class="display-math">$$ z\bar w - \bar z w = z_0\bar w - \bar z_0 w. $$</span>
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-straight-line-in-the-complex-plane.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                            (Proof technique)
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Proposition (alternative form of straight line in C)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-alternative-form-of-straight-line-in-C.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Equation of the form
                                        <span class="display-math">$$ \bar w z - w\bar z = i\rho, \quad \rho\in\R $$</span>
                                        or
                                        <span class="display-math">$$ \bar w z + w\bar z = \rho, \quad \rho\in\R $$</span>
                                        represents a straight line in <span class="inline-math">$\C$</span>.
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-alternative-form-of-straight-line-in-C.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Proposition (circle in the complex plane)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-circle-in-the-complex-plane.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        For a circle with centre <span class="inline-math">$c\in\mathbb C$</span> and radius <span class="inline-math">$\rho\in\mathbb R$</span>, a general point <span class="inline-math">$z$</span> on the circle has equation
                                        <span class="display-math">$$ z\bar z - \bar cz - c\bar z = \rho - c\bar c. $$</span>
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-circle-in-the-complex-plane.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        
                    </div>
                </div>
            </p>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed">
        <div class="markdown-embed-title">
            Vectors
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Vectors.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                <div class="markdown-embed embed-definition">
                    <div class="markdown-embed-title">
                        Definition (vector)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-vector.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            A vector <span class="inline-math">$\vec v$</span> has a length and direction such that
                            <span class="display-math">$$ |\vec v|=0 \iff \vec v=\vec 0. $$</span>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Notation (unit vector)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Notation-unit-vector.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            If <span class="inline-math">$|\vec v| = 1$</span> then it is written as <span class="inline-math">$\hat v$</span>, called the unit vector.
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed embed-definition">
                    <div class="markdown-embed-title">
                        Definition (vector field)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-vector-field.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            A vector field <span class="inline-math">$\vec f = \vec f(\vec x)$</span> is a vector function of position and maybe time.
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed embed-definition">
                    <div class="markdown-embed-title">
                        Definition (vector space)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-vector-space.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            A vector space <span class="inline-math">$V$</span> is a set of vectors equipped with a scalar field, closed under vector addition and scalar multiplication, and satisfy,
                        </p>
                        <ul>
                        <li>
                            <p>
                                for vector addition,
                            </p>
                            <ul>
                            <li>
                                <p>
                                    commutativity: <span class="inline-math">$\vec a+\vec b = \vec b+\vec a$</span>;
                                </p>
                                
                            </li><li>
                                <p>
                                    associativity: <span class="inline-math">$(\vec a + \vec b) + \vec c = \vec a + (\vec b + \vec c)$</span>;
                                </p>
                                
                            </li><li>
                                <p>
                                    identity: there exists a unique <span class="inline-math">$\vec 0\in V$</span> such that <span class="inline-math">$\vec a + \vec 0 = \vec a$</span> for all <span class="inline-math">$\vec a\in V$</span>;
                                </p>
                                
                            </li><li>
                                <p>
                                    inverses: for all <span class="inline-math">$a\in V$</span>, there exists a unique additive inverse <span class="inline-math">$-\vec a$</span> such that <span class="inline-math">$\vec a+(-\vec a) = \vec 0$</span>,
                                </p>
                                
                            </li>
                            </ul>
                            
                        </li><li>
                            <p>
                                and for scalar multiplication,
                            </p>
                            <ul>
                            <li>
                                <p>
                                    distributive properties:
                                </p>
                                <ul>
                                <li>
                                    <p>
                                        <span class="inline-math">$(\lambda+\mu)\vec v = \lambda\vec v+\mu\vec v$</span>;
                                    </p>
                                    
                                </li><li>
                                    <p>
                                        <span class="inline-math">$\lambda(\vec u+\vec v) = \lambda\vec u + \lambda\vec v$</span>;
                                    </p>
                                    
                                </li>
                                </ul>
                                
                            </li><li>
                                <p>
                                    Associativity: <span class="inline-math">$\lambda(\mu\vec v) = (\lambda\mu)\vec v$</span>;
                                </p>
                                
                            </li><li>
                                <p>
                                    Identity: <span class="inline-math">$1\vec v = \vec v$</span>.
                                </p>
                                
                            </li>
                            </ul>
                            
                        </li>
                        </ul>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Products of vectors
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Products-of-vectors.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Scalar product
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Scalar-product.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        <div class="markdown-embed embed-definition">
                                            <div class="markdown-embed-title">
                                                Definition (geometric definition of the dot product)
                                                <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-geometric-definition-of-the-dot-product.html">open_in_new</a>
                                            </div>
                                            <div class="markdown-embed-content">
                                                <p>
                                                    For <span class="inline-math">$\vec a,\vec b\in\mathbb R^2$</span> or <span class="inline-math">$\mathbb R^3$</span>, define
                                                    <span class="display-math">$$ \vec a\cdot\vec b = |\vec a||\vec b|\cos\theta $$</span>
                                                    where <span class="inline-math">$\theta$</span> is the angle between <span class="inline-math">$\vec a$</span> and <span class="inline-math">$\vec b$</span>.
                                                </p>
                                                
                                            </div>
                                        </div>
                                    </p>
                                    <p>
                                        <div class="markdown-embed embed-definition">
                                            <div class="markdown-embed-title">
                                                Definition (projection)
                                                <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-projection.html">open_in_new</a>
                                            </div>
                                            <div class="markdown-embed-content">
                                                <p>
                                                    <span class="inline-math">$\vec b^\perp$</span> is the projection of <span class="inline-math">$\vec b$</span> onto <span class="inline-math">$\vec a$</span> if
                                                    <span class="display-math">$$ \vec b^\perp = (\vec b\cdot\hat a)\hat a. $$</span>
                                                </p>
                                                
                                            </div>
                                        </div>
                                    </p>
                                    <p>
                                        <div class="markdown-embed embed-definition">
                                            <div class="markdown-embed-title">
                                                Definition (inner product)
                                                <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-inner-product.html">open_in_new</a>
                                            </div>
                                            <div class="markdown-embed-content">
                                                <p>
                                                    For a real vector space <span class="inline-math">$V$</span>, the inner product of <span class="inline-math">$x,y\in V$</span>, denoted as <span class="inline-math">$\vec x\cdot\vec y$</span> or <span class="inline-math">$\langle x|y\rangle$</span>, satisfies
                                                </p>
                                                <ul>
                                                <li>
                                                    <p>
                                                        commutativity: <span class="inline-math">$\vec x\cdot\vec y = \vec y\cdot\vec x$</span> (only true for real vectors);
                                                    </p>
                                                    
                                                </li><li>
                                                    <p>
                                                        linearity of the second argument: <span class="inline-math">$\vec x\cdot(\lambda\vec y + \mu\vec z) = \lambda\vec x\cdot\vec y + \mu\vec x\cdot\vec z$</span>;
                                                    </p>
                                                    
                                                </li><li>
                                                    <p>
                                                        <a href="/tripos-notes/Lectures/Michaelmas/Differential-Equations-DE/Definition-positive-definite-negative-definite-indefinite.html">positive definite</a>: <span class="inline-math">$\vec x\cdot\vec x\geq 0$</span> with equality iff <span class="inline-math">$\vec x=0$</span>.
                                                    </p>
                                                    
                                                </li>
                                                </ul>
                                                <p>
                                                    The first two properties give the linearity of the first argument:
                                                    <span class="inline-math">$(\lambda\vec y+\mu\vec z)\cdot\vec x = \lambda\vec y\cdot\vec x + \mu\vec z\cdot\vec x$</span>.
                                                </p>
                                                
                                            </div>
                                        </div>
                                    </p>
                                    <p>
                                        <div class="markdown-embed embed-result">
                                            <div class="markdown-embed-title">
                                                Theorem (Cauchy-Schwarz inequality)
                                                <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Theorem-Cauchy-Schwarz-inequality.html">open_in_new</a>
                                            </div>
                                            <div class="markdown-embed-content">
                                                <p>
                                                    For all <span class="inline-math">$\vec x,y\in\mathbb R^n$</span>,
                                                    <span class="display-math">$$\vec x\cdot\vec y\leq|\vec x||\vec y|$$</span>
                                                    with equality iff <span class="inline-math">$\vec x=\vec 0$</span> or <span class="inline-math">$\vec y=\vec 0$</span> or <span class="inline-math">$\vec x=\lambda\vec y$</span> for some <span class="inline-math">$\lambda\in\mathbb R$</span>.
                                                </p>
                                                <p>
                                                    <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-Cauchy-Schwarz-inequality.html">Proof</a>
                                                </p>
                                                
                                            </div>
                                        </div>
                                    </p>
                                    <p>
                                        The triangle inequality is a consequence of the C-S inequality:
                                        <div class="markdown-embed embed-result">
                                            <div class="markdown-embed-title">
                                                Proposition (triangle inequality for R^n)
                                                <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-triangle-inequality-for-R-n.html">open_in_new</a>
                                            </div>
                                            <div class="markdown-embed-content">
                                                <p>
                                                    For all <span class="inline-math">$\vec x,\vec y\in\mathbb R^n$</span>,
                                                    <span class="display-math">$$ |\vec x+\vec y|\leq |\vec x|+|\vec y|. $$</span>
                                                </p>
                                                <p>
                                                    <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-triangle-inequality-for-R-n.html">Proof</a>
                                                </p>
                                                
                                            </div>
                                        </div>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Vector product
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Vector-product.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        <div class="markdown-embed embed-definition">
                                            <div class="markdown-embed-title">
                                                Definition (vector product)
                                                <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-vector-product.html">open_in_new</a>
                                            </div>
                                            <div class="markdown-embed-content">
                                                <p>
                                                    For <span class="inline-math">$\vec a,\vec b\in\mathbb R^3$</span>, define
                                                    <span class="display-math">$$ \vec a\times\vec b = |\vec a||\vec b|\sin\theta\ \hat n $$</span>
                                                    where <span class="inline-math">$\theta$</span> is the angle between <span class="inline-math">$\vec a$</span> and <span class="inline-math">$\vec b$</span>, and <span class="inline-math">$\hat n$</span> is the unit vector perpendicular to <span class="inline-math">$\vec a,\vec b$</span> in the right-handed sense.
                                                </p>
                                                
                                            </div>
                                        </div>
                                    </p>
                                    <p>
                                        Properties of the vector product:
                                    </p>
                                    <ul>
                                    <li>
                                        <p>
                                            <span class="inline-math">$\vec a\times\vec b = -\vec b\times\vec a$</span>;
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            <span class="inline-math">$\vec a\times\vec 0 = \vec 0$</span>;
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            <span class="inline-math">$\vec a\times\vec b = \vec 0 \implies \vec a = \mu\vec b, \mu\in\mathbb R$</span>;
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            <span class="inline-math">$\vec a\times(\lambda\vec b) = \lambda(\vec a \times\vec b)$</span>.
                                        </p>
                                        
                                    </li>
                                    </ul>
                                    <p>
                                        <div class="markdown-embed embed-definition">
                                            <div class="markdown-embed-title">
                                                Definition (vector area of a triangle)
                                                <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-vector-area-of-a-triangle.html">open_in_new</a>
                                            </div>
                                            <div class="markdown-embed-content">
                                                <p>
                                                    Define the vector area of a triangle <span class="inline-math">$OAB$</span> be
                                                    <span class="display-math">$$ \frac12 \vec{OA}\times\vec{OB}.$$</span>
                                                </p>
                                                
                                            </div>
                                        </div>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Scalar triple product
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Scalar-triple-product.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        <div class="markdown-embed">
                                            <div class="markdown-embed-title">
                                                Notation (scalar triple product)
                                                <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Notation-scalar-triple-product.html">open_in_new</a>
                                            </div>
                                            <div class="markdown-embed-content">
                                                <p>
                                                    Define
                                                    <span class="display-math">$$[\vec a,\vec b,\vec c] = \vec a\cdot(\vec b\times\vec c).$$</span>
                                                </p>
                                                
                                            </div>
                                        </div>
                                    </p>
                                    <p>
                                        This is the volume of the parallelepiped formed by <span class="inline-math">$\vec a,\vec b,\vec c$</span>.
                                    </p>
                                    <p>
                                        Properties:
                                    </p>
                                    <ul>
                                    <li>
                                        <p>
                                            <span class="inline-math">$[\vec a,\vec b,\vec c] = [\vec b,\vec c,\vec a] = [\vec c,\vec a,\vec b] = -[\vec a, \vec c, \vec b] = -[\vec b, \vec a, \vec c] = -[\vec c,\vec b,\vec a]$</span>;
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            <span class="inline-math">$[\vec a,\vec a,\vec c] = [\vec a,\vec b,\vec b], [\vec a,\vec b,\vec a] = 0$</span>.
                                        </p>
                                        
                                    </li>
                                    </ul>
                                    <p>
                                        The condition for <span class="inline-math">$\vec a,\vec b, \vec c$</span> to be coplanar is
                                        <span class="display-math">$$[\vec a,\vec b,\vec c] = 0.$$</span>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Spanning sets and bases
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Spanning-sets-and-bases.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            <div class="markdown-embed embed-definition">
                                <div class="markdown-embed-title">
                                    Definition (spanning set)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-spanning-set.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        For a vector space <span class="inline-math">$V$</span>, we say <span class="inline-math">$\{\vec v_1, \vec v_2, \ldots, \vec v_n\}$</span> spans <span class="inline-math">$V$</span>, or it is a spanning set of <span class="inline-math">$V$</span>, when every vector <span class="inline-math">$\vec v\in V$</span> can be written as a linear combination of <span class="inline-math">$\vec v_1,\ldots, \vec v_n$</span>, i.e. there exists scalars <span class="inline-math">$c_1,\ldots,c_n$</span> such that
                                        <span class="display-math">$$ \vec v = \sum_{i=1}^n c_i \vec v_i. $$</span>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-definition">
                                <div class="markdown-embed-title">
                                    Definition (linear independence)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-linear-independence.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        A list of vectors <span class="inline-math">$\vec v_1,\vec v_2,\ldots, \vec v_n\in V$</span> is linearly independent when for some scalars <span class="inline-math">$c_1,\ldots, c_n$</span>,
                                        <span class="display-math">$$ \sum_i c_i\vec v_i = \vec 0 \implies c_i=0\quad\forall i.$$</span>
                                    </p>
                                    <p>
                                        If it is not linearly independent, then it is linearly dependent.
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-definition">
                                <div class="markdown-embed-title">
                                    Definition (basis)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-basis.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        A list of vectors in a vector space <span class="inline-math">$V$</span> is called a basis of <span class="inline-math">$V$</span> if it
                                    </p>
                                    <ul>
                                    <li>
                                        <p>
                                            spans <span class="inline-math">$V$</span>; and
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            is linearly independent.
                                        </p>
                                        
                                    </li>
                                    </ul>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-definition">
                                <div class="markdown-embed-title">
                                    Definition (dimension)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-dimension.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        The dimension of a vector space is the number of vectors in the basis.
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Proposition (uniqueness of linear combination of basis)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-uniqueness-of-linear-combination-of-basis.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        If <span class="inline-math">$\vec u_1,\vec u_2,\ldots,\vec u_n$</span> is a basis of a vector space <span class="inline-math">$V$</span>, then for any vector <span class="inline-math">$\vec v\in V$</span>,
                                        <span class="inline-math">$\vec v$</span> written as a linear combination of basis is unique.
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-uniqueness-of-linear-combination-of-basis.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Real and complex coordinate space
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Real-and-complex-coordinate-space.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Proposition (real coordinate space)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-real-coordinate-space.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        <span class="inline-math">$\mathbb R^n = \{ (x_1,x_2,\ldots,x_n): x_j\in\mathbb R, j=1,2,\ldots, n\}$</span> is a vector space.
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-real-coordinate-space.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Proposition (linear independence in R2)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-linear-independence-in-R2.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        If two vectors <span class="inline-math">$\vec a,\vec b\neq\vec 0$</span> in <span class="inline-math">$\mathbb R^2$</span> satisfy <span class="inline-math">$\vec a\times\vec b\neq \vec 0$</span>, 
                                        <span class="inline-math">$\vec a$</span> and <span class="inline-math">$\vec b$</span> are linearly independent.
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-linear-independence-in-R2.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Proposition (bases of R2)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-bases-of-R2.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Any two vectors <span class="inline-math">$\vec a,\vec b\neq\vec 0$</span> in <span class="inline-math">$\mathbb R^2$</span> with <span class="inline-math">$\vec a\times\vec b\neq \vec 0$</span> form a basis of <span class="inline-math">$\mathbb R^2$</span>.
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Proposition (linear independence in R3)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-linear-independence-in-R3.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        If <span class="inline-math">$\vec a, \vec b, \vec c\in\mathbb R^3$</span> are not coplanar, then they are linearly independent.
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-linear-independence-in-R3.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Proposition (bases of R3)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-bases-of-R3.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        If <span class="inline-math">$\vec a, \vec b, \vec c\in\mathbb R^3$</span> are not coplanar, then they form a basis of <span class="inline-math">$\mathbb R^3$</span>.
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Standard basis of Rn
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Standard-basis-of-Rn.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        The standard basis of <span class="inline-math">$\mathbb R^n$</span> is <span class="inline-math">$\{\vec e_1,\vec e_2,\ldots, \vec e_n\}$</span> where
                                        <span class="display-math">$$ \vec e_1 = (1,0,\ldots,0), \quad\vec e_2 = (0,1,\ldots,0),\quad \ldots\quad\vec e_n = (0,\ldots,0,1). $$</span>
                                    </p>
                                    <p>
                                        This basis is <em>orthonormal</em>, i.e.
                                        <span class="display-math">$$ \vec e_i\cdot\vec e_j = \begin{cases}
                                        0, &amp;i\neq j &amp;\text{(orthogonal)}\\
                                        1, &amp;i=j.
                                        \end{cases}$$</span>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-definition">
                                <div class="markdown-embed-title">
                                    Definition (inner product of Rn)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-inner-product-of-Rn.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        For any two vectors <span class="inline-math">$\vec x,\vec y\in\mathbb R^n$</span> where
                                        <span class="display-math">$$\begin{align*}
                                        \vec x &amp;= \sum_{i=1}^n x_i\vec e_i \\
                                        \vec y &amp;= \sum_{i=1}^n y_i\vec e_i,
                                        \end{align*}$$</span>
                                        define its dot or inner product
                                        <span class="display-math">$$ \vec x\cdot\vec y = \langle \vec x | \vec y\rangle = \sum_{i=1}^n x_iy_i. $$</span>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-definition">
                                <div class="markdown-embed-title">
                                    Definition (inner product of Cn)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-inner-product-of-Cn.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        For any vectors <span class="inline-math">$\vec u,\vec v\in\mathbb C^n$</span> where
                                        <span class="display-math">$$ \vec u = \sum_{i=1}^n u_i \vec e_i, \quad u_i\in\mathbb C $$</span>
                                        <span class="display-math">$$ \vec v = \sum_{i=1}^n v_i \vec e_i, \quad v_i\in\mathbb C $$</span>
                                        define their dot or inner product
                                        <span class="display-math">$$ \vec u\cdot\vec v = \langle\vec u|\vec v\rangle = \sum_{i=1}^n\overline{u_i} v_i.$$</span>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Properties of complex inner product
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Properties-of-complex-inner-product.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <ul>
                                    <li>
                                        <p>
                                            <span class="inline-math">$\vec u\cdot\vec v = \overline{\vec v\cdot\vec u}$</span>;
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            <span class="inline-math">$\vec u\cdot\vec u\geq 0$</span>, and <span class="inline-math">$\vec u\cdot\vec u=0\implies\vec u=\vec 0$</span>.
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            <span class="inline-math">$\vec u\cdot(\lambda\vec v+\mu\vec w) = \lambda\vec u\cdot\vec v + \mu\vec u\cdot\vec w$</span> (linearity of the second argument);
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            <span class="inline-math">$(\lambda\vec u+\mu\vec v)\cdot\vec w = \overline\lambda\vec u\cdot\vec w + \overline\mu\vec v\cdot\vec w$</span> (<a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Distributing-first-argument-of-complex-dot-product.html">Proof</a>).
                                        </p>
                                        
                                    </li>
                                    </ul>
                                    
                                </div>
                            </div>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Vector subspace
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Vector-subspace.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            <div class="markdown-embed embed-definition">
                                <div class="markdown-embed-title">
                                    Definition (subspace)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-subspace.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        A non-empty subset <span class="inline-math">$U$</span> of a vector space <span class="inline-math">$V$</span> is a subspace of <span class="inline-math">$V$</span> if <span class="inline-math">$U$</span> is a vector space under the same operations as <span class="inline-math">$V$</span>.
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-example">
                                <div class="markdown-embed-title">
                                    Examples of subspace
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Examples-of-subspace.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <ul>
                                    <li>
                                        <p>
                                            <span class="inline-math">$V$</span> is a subspace of <span class="inline-math">$V$</span>.
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            <span class="inline-math">$\{\vec 0\}$</span> is a subspace of <span class="inline-math">$V$</span>.
                                        </p>
                                        
                                    </li>
                                    </ul>
                                    <p>
                                        All others are proper subspaces.
                                    </p>
                                    <ul>
                                    <li>
                                        <p>
                                            For any <span class="inline-math">$\vec\alpha\in\R^n$</span>,  let
                                            <span class="display-math">$$ U = \sb{\vec x\in\R^n}{\vec\alpha\cdot\vec x = 0}. $$</span>
                                            <span class="inline-math">$U$</span> is a subspace of <span class="inline-math">$\R^n$</span> and has a dimension of <span class="inline-math">$n-1$</span>.
                                        </p>
                                        
                                    </li>
                                    </ul>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Theorem (subspace condition)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Theorem-subspace-condition.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        A non-empty subset <span class="inline-math">$U$</span> of a vector space <span class="inline-math">$V$</span> is a subspace of <span class="inline-math">$V$</span> iff for all <span class="inline-math">$\vec x,\vec y\in U$</span> and scalars <span class="inline-math">$\lambda,\mu$</span>,
                                        <span class="display-math">$$\lambda\vec x + \mu\vec y \in U.$$</span>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Einstein suffix notation
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Einstein-suffix-notation.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Notation (Einstein suffix notation)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Notation-Einstein-suffix-notation.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        For a vector <span class="inline-math">$\vec v\in\R^3$</span>, denote <span class="inline-math">$v_i$</span> where <span class="inline-math">$i=1,2,3$</span> be the <span class="inline-math">$i$</span>-th component of <span class="inline-math">$\vec v$</span>, i.e.
                                        <span class="display-math">$$ \vec v = v_1\vec e_1 + v_2\vec e_2 + v_3\vec e_3. $$</span>
                                    </p>
                                    <p>
                                        In Einstein suffix notation, if in the same term, a suffix appears
                                    </p>
                                    <ul>
                                    <li>
                                        <p>
                                            once, then it is a free suffix;
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            twice, then it is a dummy suffix, and summation over it is omitted.
                                        </p>
                                        
                                    </li>
                                    </ul>
                                    <p>
                                        A suffix cannot appears more than twice.
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Dot product in suffix notation
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Dot-product-in-suffix-notation.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        <span class="display-math">$$ \vec x\cdot\vec y = x_i y_i. $$</span>
                                        Since <span class="inline-math">$i$</span> is a dummy suffix, the summation sign <span class="inline-math">$\sum_{i=1}^3$</span> is omitted.
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-definition">
                                <div class="markdown-embed-title">
                                    Definition (Kronecker delta)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-Kronecker-delta.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        <span class="inline-math">$\delta_{ij}$</span> is a 2-dimensional tensor with 2 free suffixes, defined as
                                        <span class="display-math">$$ \delta_{ij} = \begin{cases}1, &amp;i=j, \\0, &amp;i\neq j.\end{cases} $$</span>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Properties of the Kronecker delta
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Properties-of-the-Kronecker-delta.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <ul>
                                    <li>
                                        <p>
                                            <span class="inline-math">$\delta_{ij} = \delta_{ji}$</span> (symmetric);
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            <span class="inline-math">$\delta_{ii} = 3$</span> in <span class="inline-math">$\R^3$</span>;
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            <span class="inline-math">$a_i\delta_{ij} = a_j$</span>;
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            <span class="inline-math">$\delta_{ij}\delta_{jk} = \delta_{ik}$</span>;
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            <span class="inline-math">$a_i \delta_{ij} b_j = a_ib_i = \vec a\cdot\vec b$</span>.
                                        </p>
                                        
                                    </li>
                                    </ul>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-definition">
                                <div class="markdown-embed-title">
                                    Definition (Levi-Cevita symbol)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-Levi-Cevita-symbol.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Define a permutation of <span class="inline-math">$\se{1,2,3}$</span> as even (or odd) if an even (or odd) number of swaps are needed to achieve the permutation from <span class="inline-math">$(1,2,3)$</span>.
                                    </p>
                                    <p>
                                        Then define the Levi-Cevita (alternating) symbol
                                        <span class="display-math">$$ \epsilon_{ijk} = \begin{cases}
                                        +1, &amp;ijk\text{ is a even permutation}, \\
                                        -1, &amp;ijk\text{ is an odd permutation}, \\
                                        0, &amp;ijk\text{ is not a permutation}.
                                        \end{cases} $$</span>
                                        This means
                                        <span class="display-math">$$\begin{align*}
                                        \epsilon_{123} = \epsilon_{231} = \epsilon_{312} &amp;= +1, \\
                                        \epsilon_{132} = \epsilon_{213} = \epsilon_{321} &amp;= -1, \\
                                        \text{Others} &amp;= 0.
                                        \end{align*}$$</span>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Properties of the Levi-Cevita symbol
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Properties-of-the-Levi-Cevita-symbol.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <ul>
                                    <li>
                                        <p>
                                            <span class="inline-math">$\epsilon_{ijk}\delta_{jk} = 0$</span>;
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            If <span class="inline-math">$a_{jk} = a_{kj}$</span>, then <span class="inline-math">$\epsilon_{ijk}a_{jk} = 0$</span>. <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-symmetric-symbol-times-epsilon.html">Proof</a>
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            <span class="inline-math">$(\vec a\times\vec b)_i = \epsilon_{ijk} a_j b_k$</span>.
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            <span class="inline-math">$\epsilon_{ijk} \epsilon_{ij&#x27;k&#x27;} = \delta_{jj&#x27;}\delta_{kk&#x27;} - \delta_{jk&#x27;}\delta_{kj&#x27;}$</span> <br>
                                            <span class="inline-math">$\epsilon_{ijk} \epsilon_{i&#x27;jk&#x27;} = \delta_{ii&#x27;}\delta_{kk&#x27;} - \delta_{ik&#x27;}\delta_{ki&#x27;}$</span> <br>
                                            <span class="inline-math">$\epsilon_{ijk} \epsilon_{i&#x27;j&#x27;k} = \delta_{ii&#x27;}\delta_{jj&#x27;} - \delta_{ij&#x27;}\delta_{ji&#x27;}$</span>.
                                            [[FILE NOT FOUND: Proof (multiplying two alternating symbols)]]
                                        </p>
                                        
                                    </li>
                                    </ul>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Proposition (vector triple product)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-vector-triple-product.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        For <span class="inline-math">$\vec a,\vec b, \vec c\in\R^3$</span>,
                                        <span class="display-math">$$\begin{align*}
                                        \vec a\times(\vec b\times\vec c) &amp;= (\vec a\cdot\vec c)\vec b - (\vec a\cdot\vec b)\vec c \\
                                        (\vec a\times\vec b)\times\vec c &amp;= (\vec a\cdot\vec c)\vec b - (\vec b\cdot\vec c)\vec a.
                                        \end{align*}$$</span>
                                        (Furthest dot product is positive)
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-vector-triple-product.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Geometry in R3
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Geometry-in-R3.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Line in R3
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Line-in-R3.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        If a line <span class="inline-math">$L$</span> goes through a point <span class="inline-math">$A$</span> with position vector <span class="inline-math">$\vec a$</span> and is parallel to <span class="inline-math">$\vec t$</span>, a general point <span class="inline-math">$\vec x$</span> on <span class="inline-math">$L$</span> is
                                        <span class="display-math">$$ \vec x = \vec a + \lambda\vec t,\quad \lambda\in\R $$</span>
                                        or
                                        <span class="display-math">$$  (\vec x - \vec a)\times\vec t = \vec 0. $$</span>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Plane in R3
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Plane-in-R3.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        If a plane has a normal vector <span class="inline-math">$\vec n$</span> and contains a point <span class="inline-math">$\vec b$</span>, then <span class="inline-math">$(\vec x -\vec b)$</span> lies on the plane, so
                                        <span class="display-math">$$ (\vec x - \vec b)\cdot\vec n = 0. $$</span>
                                    </p>
                                    <p>
                                        For <span class="inline-math">$\vec n=\hat n$</span>, then <span class="inline-math">$\vec b\cdot\hat n\equiv p$</span> is the perpendicular distance from origin to plane,
                                        <span class="display-math">$$ \vec x\cdot\hat n = p. $$</span>
                                    </p>
                                    <p>
                                        Alternatively, if <span class="inline-math">$\vec a, \vec b, \vec c$</span> lie on the plane, then
                                        <span class="display-math">$$ (\vec x - \vec a) \cdot \big((\vec b-\vec a)\times(\vec c-\vec a)\big) = 0. $$</span>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Example-intersection-of-a-line-and-a-plane.html">Example (intersection of a line and a plane)</a>
                        </p>
                        <p>
                            <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Example-shortest-distance-between-two-lines.html">Example (shortest distance between two lines)</a>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Vector equation
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Vector-equation.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            The strategy is to take dot/cross product with <em>suitable</em> vectors.
                        </p>
                        <p>
                            <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Example-of-a-vector-equation.html">Example of a vector equation</a>
                        </p>
                        
                    </div>
                </div>
            </p>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed">
        <div class="markdown-embed-title">
            Matrices and linear maps
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Matrices-and-linear-maps.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                <div class="markdown-embed embed-example">
                    <div class="markdown-embed-title">
                        Example (rotation matrix)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Example-rotation-matrix.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            To rotate a vector by <span class="inline-math">$\theta$</span> about axis <span class="inline-math">$\hat n$</span>,
                            the matrix is
                            <span class="display-math">$$ R_{ij} = \delta_{ij}\cos\theta + (1-\cos\theta)n_in_j - \epsilon_{ijk}n_k\sin\theta. $$</span>
                        </p>
                        <p>
                            <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Derivation-rotation-matrix.html">Derivation</a>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed embed-example">
                    <div class="markdown-embed-title">
                        Example (reflection and projection onto plane)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Example-reflection-and-projection-onto-plane.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            To reflect a point in <span class="inline-math">$\R^3$</span> about a plane normal to <span class="inline-math">$\hat n$</span> through the origin, the matrix is
                            <span class="display-math">$$ R_{ij} = \delta_{ij} - 2n_in_j. $$</span>
                        </p>
                        <p>
                            To project it onto the plane, the matrix is
                            <span class="display-math">$$ R_{ij} = \delta_{ij} - n_in_j. $$</span>
                        </p>
                        <p>
                            <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Derivation-reflection-and-projection-onto-plane.html">Derivation</a>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed embed-definition">
                    <div class="markdown-embed-title">
                        Definition (linear map)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-linear-map.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            Let <span class="inline-math">$V,W$</span> be vector spaces and <span class="inline-math">$T: V\to W$</span>. <span class="inline-math">$T$</span> is a linear map if
                        </p>
                        <ul>
                        <li>
                            <p>
                                <span class="inline-math">$T(\vec a + \vec b) = T(\vec a) + T(\vec b)$</span> for all <span class="inline-math">$\vec a,\vec b\in V$</span>; and
                            </p>
                            
                        </li><li>
                            <p>
                                <span class="inline-math">$T(\lambda\vec a) = \lambda T(\vec a)$</span> for all scalar <span class="inline-math">$\lambda$</span> and <span class="inline-math">$\vec a\in V$</span>.
                            </p>
                            
                        </li>
                        </ul>
                        <p>
                            Or equivalently, for all scalars <span class="inline-math">$\lambda,\mu$</span> and vectors <span class="inline-math">$\vec a,\vec b\in V$</span>,
                            <span class="display-math">$$ T(\lambda\vec a + \mu\vec b) = \lambda T(\vec a) + \mu T(\vec b). $$</span>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed embed-definition">
                    <div class="markdown-embed-title">
                        Definition (image and kernel of linear map)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-image-and-kernel-of-linear-map.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            Consider a linear map <span class="inline-math">$f: U\to V$</span>.
                        </p>
                        <ul>
                        <li>
                            <p>
                                The image of <span class="inline-math">$f$</span>, denoted as <span class="inline-math">$f(U)$</span> or <span class="inline-math">$\mrm{image}(f)$</span>, is the set
                                <span class="display-math">$$ \sb{f(\vec u)}{\vec u\in U}. $$</span>
                            </p>
                            
                        </li><li>
                            <p>
                                The kernel of <span class="inline-math">$f$</span>, denoted as <span class="inline-math">$\mrm{kernel}(f)$</span>, is the set
                                <span class="display-math">$$ \sb{\vec u\in U}{f(\vec u) = \vec 0}. $$</span>
                            </p>
                            
                        </li>
                        </ul>
                        
                    </div>
                </div>
            </p>
            <p>
                <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Example-of-computing-image-and-kernel.html">Example of computing image and kernel</a>
            </p>
            <p>
                <div class="markdown-embed embed-result">
                    <div class="markdown-embed-title">
                        Theorem (image and kernel are subspaces)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Theorem-image-and-kernel-are-subspaces.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            Consider the linear map <span class="inline-math">$f: U\to V$</span> where <span class="inline-math">$U, V$</span> are vector spaces.
                        </p>
                        <ul>
                        <li>
                            <p>
                                <span class="inline-math">$\mrm{image}(f)$</span> is a subspace of <span class="inline-math">$V$</span>.
                            </p>
                            
                        </li><li>
                            <p>
                                <span class="inline-math">$\mrm{kernel}(f)$</span> is a subspace of <span class="inline-math">$U$</span>.
                            </p>
                            
                        </li>
                        </ul>
                        <p>
                            <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-image-and-kernel-are-subspaces.html">Proof</a>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed embed-definition">
                    <div class="markdown-embed-title">
                        Definition (rank, nullity)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-rank-nullity.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            For a linear map <span class="inline-math">$f: U\to V$</span>,
                        </p>
                        <ul>
                        <li>
                            <p>
                                the rank of <span class="inline-math">$f$</span>, denoted as <span class="inline-math">$r(f)$</span>, is the dimension of <span class="inline-math">$f(U)$</span>; and
                            </p>
                            
                        </li><li>
                            <p>
                                the nullity of <span class="inline-math">$f$</span>, denoted as <span class="inline-math">$n(f)$</span>, is the dimension of <span class="inline-math">$\mrm{kernel}(f)$</span>.
                            </p>
                            
                        </li>
                        </ul>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed embed-result">
                    <div class="markdown-embed-title">
                        Theorem (rank-nullity)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Theorem-rank-nullity.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            For a linear map of <span class="inline-math">$f: U\to V$</span> where <span class="inline-math">$U,V$</span> are vector spaces,
                            <span class="display-math">$$ r(f) + n(f) = \dim U. $$</span>
                        </p>
                        <p>
                            <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-rank-nullity.html">Proof</a>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed embed-definition">
                    <div class="markdown-embed-title">
                        Definition (associated matrix of a linear map)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-associated-matrix-of-a-linear-map.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            Consider a linear map <span class="inline-math">$\alpha: \R^n \to \R^m$</span>. For a basis <span class="inline-math">$\vec e_1, \ldots, \vec e_n$</span> of <span class="inline-math">$\R^n$</span>, we have
                            <span class="display-math">$$\begin{align*}
                            \vec x&#x27; = \alpha(\vec x) &amp;= \alpha\Big(\sum_{j=1}^n x_j \vec e_j\Big) &amp;&amp;\text{for some scalars } x_j\in\R \\
                            &amp;= \sum_{j=1}^n x_j \alpha(\vec e_j).
                            \end{align*}$$</span>
                            In suffix notation,
                            <span class="display-math">$$ A_{ij} x_j = x&#x27;_i = \big[\alpha(\vec e_j)\big]_i x_j $$</span>
                            so let
                            <span class="display-math">$$ A_{ij} = \big[\alpha(\vec e_j)\big]_i. $$</span>
                        </p>
                        <p>
                            We write the <span class="inline-math">$m\times n$</span> matrix with <span class="inline-math">$A_{ij}$</span> being the entry at row <span class="inline-math">$i$</span> and column <span class="inline-math">$j$</span>:
                            <span class="display-math">$$ A = \{ A_{ij} \} = \begin{pmatrix}
                            A_{11} &amp; \ldots &amp; A_{1n} \\
                            \vdots &amp; A_{ij} &amp; \vdots \\
                            A_{m1} &amp; \ldots &amp; A_{mn}
                            \end{pmatrix} $$</span>
                            and we write
                            <span class="display-math">$$ \vec x&#x27; = A \vec x. $$</span>
                        </p>
                        <p>
                            Note that the <span class="inline-math">$j$</span>-th column represents what <span class="inline-math">$\alpha$</span> maps <span class="inline-math">$\vec e_j$</span> to.
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Matrix algebra
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Matrix-algebra.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Matrix addition
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Matrix-addition.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Consider two linear maps <span class="inline-math">$\alpha,\beta: \R^n\to \R^m$</span>.
                                        Define the sum of linear maps as <span class="inline-math">$(\alpha+\beta)(\vec x) = \alpha(\vec x) + \beta(\vec x)$</span>.
                                        The associated matrices satisfy
                                        <span class="display-math">$$ (A+B)_{ij} x_j = A_{ij} x_j + B_{ij} x_j $$</span>
                                        so we define matrix addition as just adding it entry by entry:
                                        <span class="display-math">$$ (A+B)_{ij} = A_{ij} + B_{ij}. $$</span>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Matrix scalar multiplication
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Matrix-scalar-multiplication.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        We define the scalar multiplication of a linear map as
                                        <span class="display-math">$$ (\lambda\alpha)(\vec x) = \lambda\cdot\alpha(\vec x). $$</span>
                                        Therefore
                                        <span class="display-math">$$ (\lambda A)_{ij} = \lambda A_{ij}. $$</span>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Matrix multiplication
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Matrix-multiplication.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Consider <span class="inline-math">$\alpha: \R^r \to \R^n$</span> and <span class="inline-math">$\beta: \R^n\to\R^m$</span>. <span class="inline-math">$A$</span> is the associated <span class="inline-math">$n\times r$</span> matrix and <span class="inline-math">$B$</span> is the associated <span class="inline-math">$m\times n$</span> matrix.
                                    </p>
                                    <p>
                                        We consider the composition map <span class="inline-math">$\beta\alpha: \R^r\to\R^m$</span>.
                                        <span class="display-math">$$\begin{align*}
                                        (BA)_{ij} x_j &amp;= \big(B(A\vec x)\big)_i \\
                                        &amp;= B_{ik}(A\vec x)_k \\
                                        &amp;= B_{ik}A_{kj}x_j
                                        \end{align*}$$</span>
                                        so we let
                                        <span class="display-math">$$ (BA)_{ij} = B_{ik}A_{kj}. $$</span>
                                    </p>
                                    <p>
                                        Properties:
                                    </p>
                                    <ul>
                                    <li>
                                        <p>
                                            In general, <span class="inline-math">$AB\neq BA$</span>.
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            Associativity: <span class="inline-math">$(AB)C = A(BC)$</span>. <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-associativity-of-matrix-multiplication.html">Proof</a>
                                        </p>
                                        
                                    </li>
                                    </ul>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Matrix transpose
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Matrix-transpose.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Let <span class="inline-math">$A$</span> be a <span class="inline-math">$m\times n$</span> matrix. The transpose of <span class="inline-math">$A$</span>, denoted as <span class="inline-math">$A^T$</span>, is a <span class="inline-math">$n\times m$</span> matrix defined by
                                        <span class="display-math">$$ (A^T)_{ij} = A_{ji}. $$</span>
                                    </p>
                                    <p>
                                        Properties:
                                    </p>
                                    <ul>
                                    <li>
                                        <p>
                                            <span class="inline-math">$(A^T)^T = A$</span>.
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            <span class="inline-math">$(AB)^T = B^T A^T$</span>. <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-transpose-of-matrix-product.html">Proof</a>
                                        </p>
                                        
                                    </li>
                                    </ul>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Matrix inverse
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Matrix-inverse.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <ul>
                                    <li>
                                        <p>
                                            <span class="inline-math">$B$</span> is the left inverse of <span class="inline-math">$A$</span> if <span class="inline-math">$BA = I$</span>.
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            <span class="inline-math">$C$</span> is the right inverse of <span class="inline-math">$A$</span> if <span class="inline-math">$AC = I$</span>.
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            If <span class="inline-math">$A$</span> is a square matrix, then <span class="inline-math">$A^{-1}\equiv B=C$</span> is the inverse of <span class="inline-math">$A$</span>. <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-left-inverse-equals-right-inverse-for-square-matrix.html">Proof</a> This means inverse <span class="inline-math">$A\inv$</span> satisfies
                                            <span class="display-math">$$ A\inv A = AA \inv = I. $$</span>
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            If <span class="inline-math">$A\inv$</span> exists, then <span class="inline-math">$A$</span> is called <em>invertible</em>.
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            <span class="inline-math">$(AB)\inv = B\inv A\inv$</span>. <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-inverse-of-matrix-product.html">Proof</a>
                                        </p>
                                        
                                    </li>
                                    </ul>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Hermitian conjugate
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Hermitian-conjugate.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        For a complex matrix <span class="inline-math">$A$</span>,
                                        <span class="display-math">$$ A^\dagger \equiv (A^T)^* \equiv (A^*)^T. $$</span>
                                    </p>
                                    <p>
                                        Properties:
                                    </p>
                                    <ul>
                                    <li>
                                        <p>
                                            <span class="inline-math">$(AB)^\dagger = B^\dagger A^\dagger$</span>.
                                        </p>
                                        
                                    </li>
                                    </ul>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Common type of matrices
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Common-type-of-matrices.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        <div class="markdown-embed embed-definition">
                                            <div class="markdown-embed-title">
                                                Definition (symmetric matrix)
                                                <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-symmetric-matrix.html">open_in_new</a>
                                            </div>
                                            <div class="markdown-embed-content">
                                                <p>
                                                    A square matrix <span class="inline-math">$A$</span> is symmetric if <span class="display-math">$$ A^T = A. $$</span>
                                                </p>
                                                
                                            </div>
                                        </div>
                                    </p>
                                    <p>
                                        <div class="markdown-embed embed-definition">
                                            <div class="markdown-embed-title">
                                                Definition (Hermitian matrix)
                                                <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-Hermitian-matrix.html">open_in_new</a>
                                            </div>
                                            <div class="markdown-embed-content">
                                                <p>
                                                    A square matrix <span class="inline-math">$A$</span> is Hermitian if
                                                    <span class="display-math">$$ A^\dagger = A. $$</span>
                                                </p>
                                                <p>
                                                    The diagonal elements are necessarily real.
                                                </p>
                                                <p>
                                                    A real symmetric matrix is Hermitian.
                                                </p>
                                                
                                            </div>
                                        </div>
                                    </p>
                                    <p>
                                        <div class="markdown-embed embed-definition">
                                            <div class="markdown-embed-title">
                                                Definition (anti-symmetric matrix)
                                                <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-anti-symmetric-matrix.html">open_in_new</a>
                                            </div>
                                            <div class="markdown-embed-content">
                                                <p>
                                                    A square matrix is anti-symmetric if
                                                    <span class="display-math">$$ A^T = -A. $$</span>
                                                    The diagonal elements are necessarily 0.
                                                </p>
                                                
                                            </div>
                                        </div>
                                    </p>
                                    <p>
                                        <div class="markdown-embed embed-definition">
                                            <div class="markdown-embed-title">
                                                Definition (skew-Hermitian matrix)
                                                <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-skew-Hermitian-matrix.html">open_in_new</a>
                                            </div>
                                            <div class="markdown-embed-content">
                                                <p>
                                                    A square matrix is skew-Hermitian <span class="inline-math">$A$</span> if
                                                    <span class="display-math">$$ A^\dagger = -A. $$</span>
                                                </p>
                                                <p>
                                                    The diagonal elements are necessarily purely imaginary.
                                                </p>
                                                <p>
                                                    A real anti-symmetric matrix is a skew-Hermitian matrix.
                                                </p>
                                                
                                            </div>
                                        </div>
                                    </p>
                                    <p>
                                        <div class="markdown-embed embed-definition">
                                            <div class="markdown-embed-title">
                                                Definition (orthogonal and unitary matrix)
                                                <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-orthogonal-and-unitary-matrix.html">open_in_new</a>
                                            </div>
                                            <div class="markdown-embed-content">
                                                <p>
                                                    A real matrix <span class="inline-math">$A$</span> is orthogonal if
                                                    <span class="display-math">$$ A^T = A\inv \quad\text{i.e. } AA^T = A^T A = I. $$</span>
                                                </p>
                                                <p>
                                                    A complex matrix <span class="inline-math">$U$</span> is unitary if
                                                    <span class="display-math">$$ U^\dagger = U\inv \quad\text{i.e. } UU^\dagger = U^\dagger U = I. $$</span>
                                                </p>
                                                
                                            </div>
                                        </div>
                                    </p>
                                    <p>
                                        <div class="markdown-embed embed-definition">
                                            <div class="markdown-embed-title">
                                                Definition (normal matrix)
                                                <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-normal-matrix.html">open_in_new</a>
                                            </div>
                                            <div class="markdown-embed-content">
                                                <p>
                                                    An <span class="inline-math">$n\times n$</span> matrix <span class="inline-math">$N$</span> is called <em>normal</em> if it commutes with its Hermitian conjugate
                                                    <span class="display-math">$$ NN^\dagger = N^\dagger N. $$</span>
                                                </p>
                                                <p>
                                                    Hermitian, symmetric, skew-Hermitian, anti-symmetric, orthogonal, unitary matrices are all normal.
                                                </p>
                                                
                                            </div>
                                        </div>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Trace of a square matrix
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Trace-of-a-square-matrix.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        For a matrix <span class="inline-math">$A$</span>, the trace of <span class="inline-math">$A$</span>, denoted as <span class="inline-math">$\mrm{trace}(A)$</span>, is the sum of the diagonal elements of <span class="inline-math">$A$</span>,
                                        <span class="display-math">$$ \mrm{trace}(A) = A_{ii}. $$</span>
                                    </p>
                                    <p>
                                        Properties:
                                    </p>
                                    <ul>
                                    <li>
                                        <p>
                                            <span class="inline-math">$\mrm{trace}(BC) = \mrm{trace}(CB)$</span>. <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-commutativity-of-trace.html">Proof</a>
                                        </p>
                                        
                                    </li>
                                    </ul>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-definition">
                                <div class="markdown-embed-title">
                                    Definition (isotropic matrix)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-isotropic-matrix.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        A matrix <span class="inline-math">$A$</span> is isotropic if
                                        <span class="display-math">$$ A = \lambda I $$</span>
                                        for some scalar <span class="inline-math">$\lambda$</span>.
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Decomposition of a square matrix
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Decomposition-of-a-square-matrix.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Any <span class="inline-math">$n\times n$</span> matrix <span class="inline-math">$B$</span> can be written as a sum of symmetric and anti-symmetric parts,
                                        <span class="display-math">$$\begin{align*}
                                        B_{ij} &amp;= \frac12 (B_{ij} + B_{ji}) + \frac12 (B_{ij} - B_{ji}) \\
                                        B &amp;= \underbrace{\frac12 (B + B^T)}_{\text{symmetric } S} + \underbrace{\frac12 (B - B^T)}_{\text{anti-symmetric } A}
                                        \end{align*}$$</span>
                                    </p>
                                    <p>
                                        A symmetric matrix <span class="inline-math">$S$</span> can be decomposed an isotropic part and a traceless part,
                                        <span class="display-math">$$\begin{align*}
                                        S_{ij} &amp;= \frac1n \mrm{trace}(S)\, \delta_{ij} + \big(S_{ij} - \frac1n \mrm{trace}(S)\, \delta_{ij} \big) \\
                                        S &amp;= \underbrace{\frac1n \mrm{trace}(S)\, I}_\text{isotropic} + \underbrace{\big(S - \frac1n\mrm{trace}(S)\, I\big)}_{\text{traceless, since } \mrm{trace}(\frac1n I) = 1}
                                        \end{align*}$$</span>
                                    </p>
                                    <p>
                                        In <span class="inline-math">$\R^3$</span>, the anti-symmetric part <span class="inline-math">$A$</span> can be written in terms of a single vector <span class="inline-math">$\vec w$</span> as
                                        <span class="display-math">$$ A_{ijk} = \epsilon_{ijk}\omega_k = \begin{pmatrix}
                                        0 &amp; \omega_3 &amp; -\omega_2 \\
                                        -\omega_3 &amp; 0 &amp; \omega_1 \\
                                        \omega_2 &amp; -\omega_1 &amp; 0
                                        \end{pmatrix} $$</span>
                                        which is a general anti-symmetric matrix.
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Proposition (rows and columns of orthogonal and unitary matrix are orthonormal)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-rows-and-columns-of-orthogonal-and-unitary-matrix-are-orthonormal.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        For an orthogonal matrix <span class="inline-math">$A$</span>, the rows of <span class="inline-math">$A$</span> form an orthonormal set. Similarly the columns of <span class="inline-math">$A$</span> also form an orthonormal set.
                                        Same for a unitary matrix.
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-rows-and-columns-of-orthogonal-and-unitary-matrix-are-orthonormal.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Gram-Schmidt process
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Gram-Schmidt-process.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Given a set <span class="inline-math">$\set{\v w_1, \v w_2, \ldots, \v w_r}$</span> of linearly independent vectors,
                                        we can use this process to form a orthonormal set <span class="inline-math">$\set{\v v_1, \v v_2, \ldots, \v w_r}$</span> that has the same span.
                                    </p>
                                    <p>
                                        Define the projection of <span class="inline-math">$\v w$</span> onto <span class="inline-math">$\v v$</span> by
                                        <span class="display-math">$$ \mathcal P_{\v v}(\v w) = \frac{\langle\v v|\v w\rangle}{\langle\v v|\v v\rangle}\v v.$$</span>
                                    </p>
                                    <ul>
                                    <li>
                                        <p>
                                            Step 1: let <span class="inline-math">$\v v_1 = \v w_1 / \sqrt{\langle\v w_1|\v w_1\rangle}$</span>.
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            Step <span class="inline-math">$k$</span>: let <span class="display-math">$$\v u_k = \v w_k - \sum_{i=1}^{k-1} \mathcal P_{\v v_i}(\v w_k), $$</span>
                                            which gives an orthogonal vector. Then normalise it by
                                            <span class="display-math">$$ \v v_k = \frac{\v u_k}{\sqrt{\langle\v u_k|\v u_k\rangle}}. $$</span>
                                            (<a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Explanation-Gram-Schmidt-process.html">Explanation</a>)
                                        </p>
                                        
                                    </li>
                                    </ul>
                                    
                                </div>
                            </div>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Determinant
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Determinant.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Motivation for determinant
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Motivation-for-determinant.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        For a linear map <span class="inline-math">$\alpha: \R^3 \to \R^3$</span> and standard basis <span class="inline-math">$\vec e_1,\vec e_2,\vec e_3\mapsto \vec e_1&#x27;, \vec e_2&#x27;, \vec e_3&#x27;$</span>,
                                        the determinant is defined to be the volume of the parallelepiped formed by <span class="inline-math">$\vec e_1&#x27;, \vec e_2&#x27;, \vec e_3&#x27;$</span>,
                                        <span class="display-math">$$\begin{align*}
                                        \det A &amp;\equiv [\vec e_1&#x27;, \vec e_2&#x27;, \vec e_3&#x27;] \\
                                        &amp;= \epsilon_{ijk} (\vec e_1&#x27;)_i (\vec e_2&#x27;)_j (\vec e_3&#x27;)_k \\
                                        &amp;= \epsilon_{ijk} \big(A_{il} (\vec e_1)_l\big) \big(A_{jm} (\vec e_2)_m\big) \big(A_{kn} (\vec e_3)_n\big) \\
                                        &amp;= \epsilon_{ijk} A_{il} \delta_{l1} A_{jm} \delta_{m2} A_{kn} \delta_{n3} \\
                                        &amp;= \epsilon_{ijk} A_{i1} A_{j2} A_{k3}.
                                        \end{align*}$$</span>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            See <a href="/tripos-notes/Lectures/Michaelmas/Groups-Gp/Permutations.html">permutations in Groups</a> for detailed discussion.
                            <div class="markdown-embed embed-definition">
                                <div class="markdown-embed-title">
                                    Definition (generalisation of Levi-Cevita symbol)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-generalisation-of-Levi-Cevita-symbol.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Define
                                        <span class="display-math">$$
                                        \epsilon_{j_1j_2\ldots j_n} = \begin{cases}
                                        +1 &amp; \text{if }(j_1,j_2,\ldots,j_n)\text{ is an even permutation}, \\
                                        -1 &amp; \text{if }(j_1,j_2,\ldots,j_n)\text{ is an odd permutation}, \\
                                        0 &amp; \text{otherwise.}
                                        \end{cases}
                                        $$</span>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-definition">
                                <div class="markdown-embed-title">
                                    Definition (determinant)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-determinant.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        For an <span class="inline-math">$n\times n$</span> matrix <span class="inline-math">$A$</span>, define its determinant
                                        <span class="display-math">$$ \det A = \sum_{\sigma\in S_n} \epsilon(\sigma) A_{\sigma(1) 1} A_{\sigma(2) 2} \ldots A_{\sigma(n) n}. $$</span>
                                    </p>
                                    <p>
                                        Alternatively, in suffix notation,
                                        <span class="display-math">$$ \det A = \epsilon_{j_1j_2\ldots j_n} A_{j_1 1} A_{j_2 2} \ldots A_{j_n n}. $$</span>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Properties of determinant
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Properties-of-determinant.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Matrices mentioned here are of size <span class="inline-math">$n\times n$</span>.
                                    </p>
                                    <p>
                                        <div class="markdown-embed embed-result">
                                            <div class="markdown-embed-title">
                                                Proposition (transpose has the same determinant)
                                                <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-transpose-has-the-same-determinant.html">open_in_new</a>
                                            </div>
                                            <div class="markdown-embed-content">
                                                <p>
                                                    <span class="display-math">$$\det A = \det A^T.$$</span>
                                                </p>
                                                <p>
                                                    <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-transpose-has-the-same-determinant.html">Proof</a>
                                                </p>
                                                
                                            </div>
                                        </div>
                                    </p>
                                    <p>
                                        <div class="markdown-embed embed-result">
                                            <div class="markdown-embed-title">
                                                Proposition (determinant when multiplying a row or column by scalar)
                                                <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-determinant-when-multiplying-a-row-or-column-by-scalar.html">open_in_new</a>
                                            </div>
                                            <div class="markdown-embed-content">
                                                <p>
                                                    If matrix <span class="inline-math">$B$</span> is formed by multiplying every entry in a single row or column of <span class="inline-math">$A$</span> by scalar <span class="inline-math">$\lambda$</span>, then <span class="display-math">$$\det B = \lambda \det A.$$</span>
                                                </p>
                                                <p>
                                                    <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-determinant-when-multiplying-a-row-or-column-by-scalar.html">More formal description and proof</a>
                                                </p>
                                                
                                            </div>
                                        </div>
                                    </p>
                                    <p>
                                        <div class="markdown-embed embed-result">
                                            <div class="markdown-embed-title">
                                                Proposition (determinant of scalar multiple)
                                                <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-determinant-of-scalar-multiple.html">open_in_new</a>
                                            </div>
                                            <div class="markdown-embed-content">
                                                <p>
                                                    <span class="display-math">$$ \det \lambda A = \lambda^n \det A. $$</span>
                                                </p>
                                                <p>
                                                    <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-determinant-of-scalar-multiple.html">Proof</a>
                                                </p>
                                                
                                            </div>
                                        </div>
                                    </p>
                                    <p>
                                        <div class="markdown-embed embed-result">
                                            <div class="markdown-embed-title">
                                                Proposition (determinant when two rows or columns equal)
                                                <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-determinant-when-two-rows-or-columns-equal.html">open_in_new</a>
                                            </div>
                                            <div class="markdown-embed-content">
                                                <p>
                                                    If two rows or columns of <span class="inline-math">$A$</span> are identical, then <span class="inline-math">$\det A = 0$</span>.
                                                </p>
                                                <p>
                                                    <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-determinant-when-two-rows-or-columns-equal.html">Proof</a>
                                                </p>
                                                
                                            </div>
                                        </div>
                                    </p>
                                    <p>
                                        <div class="markdown-embed embed-result">
                                            <div class="markdown-embed-title">
                                                Proposition (determinant when adding multiple of rows or columns onto another)
                                                <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-determinant-when-adding-multiple-of-rows-or-columns-onto-another.html">open_in_new</a>
                                            </div>
                                            <div class="markdown-embed-content">
                                                <p>
                                                    If a matrix <span class="inline-math">$B$</span> is formed by adding a scalar multiple <span class="inline-math">$\lambda$</span> of a column (or row) of <span class="inline-math">$A$</span> onto another column (or row), then <span class="inline-math">$\det B = \det A$</span>.
                                                </p>
                                                <p>
                                                    <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-determinant-when-adding-multiple-of-rows-or-columns-onto-another.html">More formal description and proof</a>
                                                </p>
                                                
                                            </div>
                                        </div>
                                    </p>
                                    <p>
                                        <div class="markdown-embed embed-result">
                                            <div class="markdown-embed-title">
                                                Proposition (determinant when two rows or columns are linearly dependent)
                                                <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-determinant-when-two-rows-or-columns-are-linearly-dependent.html">open_in_new</a>
                                            </div>
                                            <div class="markdown-embed-content">
                                                <p>
                                                    If two rows or columns of <span class="inline-math">$A$</span> are linearly dependent, then <span class="inline-math">$\det A = 0$</span>.
                                                </p>
                                                <p>
                                                    <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-determinant-when-two-rows-or-columns-are-linearly-dependent.html">Proof</a>
                                                </p>
                                                
                                            </div>
                                        </div>
                                    </p>
                                    <p>
                                        <div class="markdown-embed embed-result">
                                            <div class="markdown-embed-title">
                                                Proposition (determinant of products)
                                                <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-determinant-of-products.html">open_in_new</a>
                                            </div>
                                            <div class="markdown-embed-content">
                                                <p>
                                                    <span class="display-math">$$ \det AB = \det A \det B. $$</span>
                                                </p>
                                                <p>
                                                    <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-determinant-of-products.html">Proof</a>
                                                </p>
                                                
                                            </div>
                                        </div>
                                    </p>
                                    <p>
                                        <div class="markdown-embed embed-result">
                                            <div class="markdown-embed-title">
                                                Proposition (determinant of orthogonal and unitary matrix)
                                                <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-determinant-of-orthogonal-and-unitary-matrix.html">open_in_new</a>
                                            </div>
                                            <div class="markdown-embed-content">
                                                <ul>
                                                <li>
                                                    <p>
                                                        If <span class="inline-math">$A$</span> is orthogonal, then <span class="inline-math">$\det A = \pm 1$</span> (<a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-determinant-of-orthogonal-matrix.html">Proof</a>). In particular, in <span class="inline-math">$\R^3$</span>, <span class="inline-math">$A$</span> represents a rotation (<span class="inline-math">$\det A = 1$</span>) or a reflection (<span class="inline-math">$\det A = -1$</span>).
                                                    </p>
                                                    
                                                </li><li>
                                                    <p>
                                                        If <span class="inline-math">$U$</span> is unitary, then <span class="inline-math">$|\det U| = 1$</span> (<a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-determinant-of-unitary-matrix.html">Proof</a>).
                                                    </p>
                                                    
                                                </li>
                                                </ul>
                                                
                                            </div>
                                        </div>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Minors and cofactors
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Minors-and-cofactors.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        <div class="markdown-embed embed-definition">
                                            <div class="markdown-embed-title">
                                                Definition (minors, cofactors)
                                                <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-minors-cofactors.html">open_in_new</a>
                                            </div>
                                            <div class="markdown-embed-content">
                                                <p>
                                                    For an <span class="inline-math">$n\times n$</span> matrix <span class="inline-math">$A$</span>, define <span class="inline-math">$A^{ij}$</span> be the <span class="inline-math">$(n-1)\times(n-1)$</span> matrix in which row <span class="inline-math">$i$</span> and column <span class="inline-math">$j$</span> of <span class="inline-math">$A$</span> are removed,
                                                    <span class="display-math">$$ A^{ij} = \pmat{
                                                    A_{11} &amp; \ldots &amp; A_{1(j-1)} &amp; A_{1(j+1)} &amp; \ldots &amp; A_{1n} \\
                                                    \vdots &amp; &amp; \vdots &amp; \vdots &amp; &amp; \vdots \\
                                                    A_{(i-1)1} &amp; \ldots &amp; A_{(i-1)(j-1)} &amp; A_{(i-1)(j+1)} &amp; \ldots &amp; A_{(i-1)n} \\
                                                    A_{(i+1)1} &amp; \ldots &amp; A_{(i+1)(j-1)} &amp; A_{(i+1)(j+1)} &amp; \ldots &amp; A_{(i+1)n} \\
                                                    \vdots &amp; &amp; \vdots &amp; \vdots &amp; &amp; \vdots \\
                                                    A_{n1} &amp; \ldots &amp; A_{n(j-1)} &amp; A_{n(j+1)} &amp; \ldots &amp; A_{nn} \\
                                                    }.$$</span>
                                                </p>
                                                <ul>
                                                <li>
                                                    <p>
                                                        The minor of the <span class="inline-math">$ij$</span>-th element of <span class="inline-math">$A$</span> is defined as
                                                        <span class="display-math">$$ M_{ij} = \det A^{ij}. $$</span>
                                                    </p>
                                                    
                                                </li><li>
                                                    <p>
                                                        The cofactor of the <span class="inline-math">$ij$</span>-th element of <span class="inline-math">$A$</span> is defined as
                                                        <span class="display-math">$$ \Delta_{ij} = (-1)^{i-j} M_{ij}. $$</span>
                                                    </p>
                                                    
                                                </li>
                                                </ul>
                                                
                                            </div>
                                        </div>
                                    </p>
                                    <p>
                                        <div class="markdown-embed embed-result">
                                            <div class="markdown-embed-title">
                                                Proposition (Laplace expansion)
                                                <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-Laplace-expansion.html">open_in_new</a>
                                            </div>
                                            <div class="markdown-embed-content">
                                                <p>
                                                    For any <span class="inline-math">$I=1,2,\ldots n$</span> of an <span class="inline-math">$n\times n$</span> matrix <span class="inline-math">$A$</span>, the determinant of <span class="inline-math">$A$</span> can be expanded along column <span class="inline-math">$I$</span> as
                                                    <span class="display-math">$$ \det A = \sum_{J_I=1}^n  A_{J_I I} \Delta_{J_I I} $$</span>
                                                    or along row <span class="inline-math">$I$</span> as
                                                    <span class="display-math">$$ \det A = \sum_{J_I=1}^n A_{IJ_I} \Delta_{IJ_I}. $$</span>
                                                </p>
                                                <p>
                                                    <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-Laplace-expansion.html">Proof (Laplace expansion)</a>
                                                </p>
                                                
                                            </div>
                                        </div>
                                    </p>
                                    <p>
                                        <div class="markdown-embed embed-result">
                                            <div class="markdown-embed-title">
                                                Proposition (expanding wrongly gives zero determinant)
                                                <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-expanding-wrongly-gives-zero-determinant.html">open_in_new</a>
                                            </div>
                                            <div class="markdown-embed-content">
                                                <p>
                                                    For an <span class="inline-math">$n\times n$</span> matrix <span class="inline-math">$A$</span> and row indices <span class="inline-math">$i\neq I$</span>,
                                                    <span class="display-math">$$ \sum_{k=1}^n A_{ik} \Delta_{Ik} = 0. $$</span>
                                                    Similarly for column indices <span class="inline-math">$j\neq J$</span>,
                                                    <span class="display-math">$$ \sum_{k=1}^n A_{kj} \Delta_{kJ} = 0. $$</span>
                                                </p>
                                                <p>
                                                    <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-expanding-wrongly-gives-zero-determinant.html">Proof (expanding wrongly gives zero determinant)</a>
                                                </p>
                                                
                                            </div>
                                        </div>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        
                    </div>
                </div>
            </p>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed">
        <div class="markdown-embed-title">
            Matrices and linear equations
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Matrices-and-linear-equations.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                <div class="markdown-embed embed-result">
                    <div class="markdown-embed-title">
                        Theorem (inverse of matrix)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Theorem-inverse-of-matrix.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            For an <span class="inline-math">$n\times n$</span> matrix <span class="inline-math">$A$</span>, if <span class="inline-math">$\det A\neq 0$</span>, then <span class="inline-math">$A\inv$</span> exists and is
                            <span class="display-math">$$ (A\inv)_{ij} = \frac{\Delta_{ij}}{\det A}. $$</span>
                        </p>
                        <p>
                            <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-inverse-of-matrix.html">Proof</a>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                Calculating inverse of matrix is time-consuming.
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        System of linear equations with unique solution
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/System-of-linear-equations-with-unique-solution.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            Suppose <span class="inline-math">$A\v x = \v b$</span>, where <span class="inline-math">$A$</span> is an <span class="inline-math">$n\times n$</span> matrix, and <span class="inline-math">$\v x,\v b$</span> are <span class="inline-math">$n\times 1$</span> column vectors.
                        </p>
                        <p>
                            We call the system <em>homogenous</em> if <span class="inline-math">$\v b=\v 0$</span>, otherwise <em>inhomogenous</em>.
                        </p>
                        <p>
                            If <span class="inline-math">$\det A\neq 0$</span>, then <span class="inline-math">$A\inv$</span> <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Theorem-inverse-of-matrix.html">exists</a>, and
                            <span class="display-math">$$\v x=A\inv\v b$$</span>
                            is the unique solution.
                        </p>
                        <p>
                            Another perspective is that for the standard basis <span class="inline-math">$\v e_i$</span>, <span class="inline-math">$\det A\neq 0$</span> implies that the columns i.e. images of the basis <span class="inline-math">$\v e_i&#x27; = A\v e_i$</span> are linearly independent. Since there are <span class="inline-math">$n$</span> such vectors, <span class="inline-math">$\v e_i&#x27;$</span> form a basis of <span class="inline-math">$\R^n$</span> or <span class="inline-math">$\C^n$</span>. Therefore,
                            <span class="display-math">$$ \text{rank } r(A) = n,\quad \text{nullity } n(A) = 0. $$</span>
                            This agrees with the fact that the kernel is <span class="inline-math">$\set \v0$</span>.
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Gaussian elimination
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Gaussian-elimination.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Process of Gaussian elimination
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Process-of-Gaussian-elimination.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        For a system of <span class="inline-math">$m$</span> linear equations in <span class="inline-math">$n$</span> unknowns,
                                        <span class="display-math">$$\pmat{A_{11} &amp; A_{12} &amp; A_{13} &amp; \ldots &amp; A_{1n} \\
                                        A_{21} &amp; A_{22} &amp; A_{23} &amp; \ldots &amp; A_{2n} \\
                                        \vdots &amp; &amp; &amp; \ddots &amp; \vdots \\
                                        A_{m1} &amp; A_{m2} &amp; A_{m3} &amp; \ldots &amp; A_{mn}}
                                        \pmat{x_1 \\ x_2 \\ x_3 \\ \vdots \\ x_n}
                                        =
                                        \pmat{d_1 \\ d_2 \\ \vdots \\ d_m}
                                        
                                        \qquad
                                        \qquad
                                        
                                        \begin{cases}
                                        A_{11} x_1 + A_{12} x_2 + \ldots + A_{1n} x_n &amp;= d_1 \\
                                        A_{21} x_1 + A_{22} x_2 + \ldots + A_{2n} x_n &amp;= d_2 \\
                                        \vdots \\
                                        A_{m1} x_1 + A_{m2} x_2 + \ldots + A_{mn} x_n &amp;= d_m \\
                                        \end{cases}$$</span>
                                    </p>
                                    <p>
                                        By assuming <span class="inline-math">$A_{11}\neq 0$</span> etc by reordering the equations or variables if needed, we write <span class="inline-math">$x_1$</span> in terms of the other variables, and then substitute into the other equations. Repeat and terminate when we have no equations with non-zero coefficients left. We will get
                                        <span class="display-math">$$\pmat{A_{11} &amp; A_{12} &amp; A_{13} &amp; \ldots &amp; \ldots &amp; \ldots &amp; A_{1n} \\
                                        0 &amp; A_{22}^{(2)} &amp; A_{23}^{(2)} &amp; \ldots &amp; \ldots &amp; \ldots &amp; A_{2n}^{(2)} \\
                                        \vdots &amp; &amp; \ddots &amp;&amp;&amp; &amp;\vdots \\
                                        0 &amp; \ldots &amp; 0 &amp; A_{rr}^{(r)} &amp; A_{r(r+1)}^{(r)} &amp; \ldots &amp; A_{rn}^{(n)} \\
                                        0 &amp; \ldots &amp; 0 &amp; 0 &amp; 0 &amp; \ldots  &amp; 0 \\
                                        \vdots &amp;&amp;&amp;&amp;&amp;&amp; \vdots \\
                                        0 &amp; \ldots &amp; \ldots &amp; \ldots &amp; \ldots &amp; \ldots &amp; 0}
                                        \pmat{x_1 \\ x_2 \\ x_3 \\ \vdots \\ x_n}
                                        =
                                        \pmat{d_1 \\ d_2 \\ \vdots \\ d_m}
                                        
                                        \quad
                                        \quad
                                        
                                        \begin{cases}
                                        A_{11} x_1 &amp;+&amp; A_{12} x_2 &amp;+&amp; \ldots &amp;+&amp;\ldots&amp;+&amp;A_{1n} x_n &amp;=&amp; d_1 \\
                                        &amp; &amp;A_{22}^{(2)} x_2 &amp;+&amp;\ldots &amp;+&amp; \ldots &amp;+&amp; A_{2n}^{(2)} x_n &amp;=&amp; d_2^{(2)} \\
                                        &amp;&amp;&amp;\ddots &amp;&amp;&amp;&amp;&amp;&amp;&amp;\vdots \\
                                        &amp;&amp;&amp;&amp;A_{rr}^{(r)} x_r &amp;+&amp; \ldots &amp;+&amp; A_{rn}^{(r)} x_n &amp;=&amp; d_r^{(r)} \\
                                        &amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;0 &amp;=&amp; d_{r+1}^{(r)} \\
                                        &amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;\vdots &amp;=&amp; \vdots \\
                                        &amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;0 &amp;=&amp; d_{n}^{(r)} \\
                                        \end{cases}$$</span>
                                        with <span class="inline-math">$A_{ii}^{(n)} \neq 0$</span> for all <span class="inline-math">$i=1,2,\ldots,r$</span>, where the superfix refers to the version number.
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Overdetermined, determined and underdetermined system
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Overdetermined-determined-and-underdetermined-system.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <ul>
                                    <li>
                                        <p>
                                            If <span class="inline-math">$r&lt;m$</span> and <span class="inline-math">$d_i^{(r)} \neq 0$</span> for some <span class="inline-math">$i=r+1,r+2,\ldots,m$</span>, then the system is <em>inconsistent</em>, so there is no solution. <br>
                                            We say the system is <em>overdetermined</em>.
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            If <span class="inline-math">$r=n\leq m$</span> and all <span class="inline-math">$d_i^{(r)} = 0$</span> for some <span class="inline-math">$i=r+1,r+2,\ldots,m$</span>, then last non-zero equation
                                            <span class="display-math">$$ A_{nn}^{(n)} x_n = d_n^{(n)} $$</span>
                                            can be solved, then performing back substitution gives a unique solution. <br>
                                            We say the system is <em>determined</em>.
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            If <span class="inline-math">$r&lt;n$</span> and all <span class="inline-math">$d_i^{(r)} = 0$</span> for some <span class="inline-math">$i=r+1,r+2,\ldots,m$</span>, then the last non-zero equation is
                                            <span class="display-math">$$A_{rr}^{(r)} x_r + A_{r(r+1)}^{(r)} x_{r+1} + \ldots + A_{rn}^{(r)} x_{n} = d_{r}^{(r)}, $$</span>
                                            so <span class="inline-math">$x_{r+1}, \ldots, x_n$</span> can be chosen freely, and back substitution gives a valid solution. <br>
                                            Therefore we have infinitely many solution. <br>
                                            We say the system is <em>underdetermined</em>.
                                        </p>
                                        
                                    </li>
                                    </ul>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Upper triangular matrix of Gaussian elimination
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Upper-triangular-matrix-of-Gaussian-elimination.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        When <span class="inline-math">$m=n$</span>,
                                        <span class="inline-math">$\det A = (-1)^R \det$</span> (the <em>upper triangular matrix</em>), where <span class="inline-math">$R$</span> is the number of rearrangements of rows and columns.
                                    </p>
                                    <p>
                                        Note that since the only <span class="inline-math">$\sigma\in S_n$</span> such that <span class="inline-math">$\sigma(i)\geq i$</span> for all <span class="inline-math">$i=1,\ldots,n$</span> is the identity,
                                        <span class="display-math">$$ \det A = (-1)^R A_{11} A_{22}^{(2)} A_{33}^{(3)} \ldots A_{nn}^{(n)}. $$</span>
                                        Therefore <span class="inline-math">$\det A=0$</span> iff <span class="inline-math">$r&lt;n$</span>.
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Matrix rank
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Matrix-rank.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            <div class="markdown-embed embed-definition">
                                <div class="markdown-embed-title">
                                    Definition (matrix rank)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-matrix-rank.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        For an <span class="inline-math">$m\times n$</span> matrix <span class="inline-math">$A$</span>, its rank is defined as <span class="inline-math">$r(A) = r(\alpha)$</span>, where <span class="inline-math">$\alpha$</span> is the associated linear map <span class="inline-math">$\alpha: \R^n \to \R^m$</span>.
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Theorem (row rank equals column rank)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Theorem-row-rank-equals-column-rank.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        For an <span class="inline-math">$m\times n$</span> matrix <span class="inline-math">$A$</span>, define the column rank of <span class="inline-math">$A$</span> as the dimension of the spanning set of columns of <span class="inline-math">$A$</span>, and similarly for row rank.
                                        Then
                                        <span class="display-math">$$ \text{row rank} = \text{column rank} = r(A). $$</span>
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-row-rank-equals-column-rank.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        General solution of linear equations
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/General-solution-of-linear-equations.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            For the equation
                            <span class="display-math">$$ A\v x = \v d $$</span>
                            where <span class="inline-math">$A$</span> is an <span class="inline-math">$m\times n$</span> matrix.
                        </p>
                        <p>
                            Let <span class="inline-math">$\set{\v u_i}_{i=1,2,\ldots,n(A)}$</span> be the basis of the kernel.
                            Extend the basis to <span class="inline-math">$\set{\v u_i}_{i=1,2,\ldots,n}$</span>. 
                            Then
                            <span class="display-math">$$ \im A = \mrm{span}\{A\v u_{n(A)+1}, \ldots A\v u_n\}.$$</span>
                        </p>
                        <ul>
                        <li>
                            <p>
                                If <span class="inline-math">$\v d \not\in \im A$</span>, then there is no solution.
                            </p>
                            
                        </li><li>
                            <p>
                                If <span class="inline-math">$\v d \in \im A$</span>, then at least one solution exists.
                                Let <span class="inline-math">$\v x_0$</span> be a particular solution.
                                The general can be written as
                                <span class="display-math">$$ \v x = \v x_0 + \v y $$</span>
                                where <span class="inline-math">$\v y \in \ker A$</span>.
                            </p>
                            <ul>
                            <li>
                                <p>
                                    If <span class="inline-math">$n(A) = 0$</span>, then <span class="inline-math">$\v y = \v 0$</span> gives the only solution.
                                </p>
                                
                            </li><li>
                                <p>
                                    If <span class="inline-math">$n(A) &gt; 0$</span>, then
                                    <span class="display-math">$$\v x = \v x_0 + \sum_{j=1}^{n(A)} \mu_j \v u_j.$$</span>
                                </p>
                                
                            </li>
                            </ul>
                            
                        </li>
                        </ul>
                        
                    </div>
                </div>
            </p>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed">
        <div class="markdown-embed-title">
            Eigenvalues and eigenvectors
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Eigenvalues-and-eigenvectors.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                <div class="markdown-embed embed-definition">
                    <div class="markdown-embed-title">
                        Definition (eigenvector, eigenvalue)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-eigenvector-eigenvalue.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            Consider linear map <span class="inline-math">$\alpha: \C^n\to\C^n$</span> with the associated <span class="inline-math">$n\times n$</span> matrix <span class="inline-math">$A$</span>.
                        </p>
                        <p>
                            A vector <span class="inline-math">$\v x\neq \v 0$</span> is an <em>eigenvector</em> of <span class="inline-math">$A$</span> and <span class="inline-math">$\lambda$</span> is the associated <em>eigenvalue</em> if
                            <span class="display-math">$$ A\v x = \lambda \v x. $$</span>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed embed-definition">
                    <div class="markdown-embed-title">
                        Definition (characteristic polynomial and equation of eigenvalues)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-characteristic-polynomial-and-equation-of-eigenvalues.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            For an <span class="inline-math">$n\times n$</span> matrix <span class="inline-math">$A$</span>, the characteristic polynomial is
                            <span class="display-math">$$ p_A(\lambda) \equiv \det(A - \lambda I). $$</span>
                            Eigenvalues satisfy the characteristic equation
                            <span class="display-math">$$ p_A(\lambda) = \det(A-\lambda I) = 0. $$</span>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Properties of the characteristic polynomial
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Properties-of-the-characteristic-polynomial.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <ul>
                        <li>
                            <p>
                                <span class="inline-math">$p_A(\lambda)$</span> is a polynomial of order <span class="inline-math">$n$</span>.
                            </p>
                            
                        </li><!-- BEGIN BLOCK ID 5b1203 -->
                        <li id="^5b1203">
                            <p>
                                By the <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Theorem-fundamental-theorem-of-algebra.html">fundamental theorem of algebra</a>, <span class="inline-math">$A$</span> has <span class="inline-math">$n$</span> eigenvalues accounting multiplicity.
                            </p>
                            
                        </li>
                        <!-- END BLOCK ID 5b1203 -->
                        <!-- BEGIN BLOCK ID ba2380 -->
                        <li id="^ba2380">
                            <p>
                                Sum and product of eigenvalues are
                                <span class="display-math">$$\begin{align*}
                                \sum_{\text{eigenvalue }\lambda} \lambda &amp;= \mrm{tr}\, A, \\
                                \prod_{\text{eigenvalue }\lambda} \lambda &amp;= \det A.
                                \end{align*}$$</span>
                                (<a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Derivation-sum-and-product-of-eigenvalues.html">Derivation</a>)
                            </p>
                            
                        </li>
                        <!-- END BLOCK ID ba2380 -->
                        
                        </ul>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed embed-definition">
                    <div class="markdown-embed-title">
                        Definition (eigenspace)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-eigenspace.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            The eigenspace <span class="inline-math">$E_\lambda$</span> of an eigenvalue <span class="inline-math">$\lambda$</span> is <span class="display-math">$$E_\lambda = \ker(A-\lambda I) = \sb{\v x\in\C^n}{A\v x= \lambda\v x}.$$</span>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed embed-definition">
                    <div class="markdown-embed-title">
                        Definition (algebraic and geometric multiplicity, degenerate, defect)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-algebraic-and-geometric-multiplicity-degenerate-defect.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            For an eigenvalue <span class="inline-math">$\lambda$</span>,
                        </p>
                        <ul>
                        <li>
                            <p>
                                The <em>algebraic multiplicity</em> <span class="inline-math">$M_\lambda$</span> is the multiplicity of <span class="inline-math">$\lambda$</span> as a root of <span class="inline-math">$p_A(\lambda) = 0$</span>.
                            </p>
                            <ul>
                            <li>
                                <p>
                                    If <span class="inline-math">$M_\lambda &gt; 1$</span>, then eigenvalue <span class="inline-math">$\lambda$</span> is called <em>degenerate</em>.
                                </p>
                                
                            </li><li>
                                <p>
                                    By the <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Theorem-fundamental-theorem-of-algebra.html">fundamental theorem of algebra</a>, <span class="inline-math">$\sum_{\text{eigenvalue }\lambda} M_\lambda = n$</span>.
                                </p>
                                
                            </li><li>
                                <p>
                                    The <em>geometric multiplicity</em> <span class="inline-math">$m_\lambda$</span> is the maximum number of linearly independent eigenvectors with eigenvalue <span class="inline-math">$\lambda$</span>, i.e.
                                    <span class="display-math">$$ m_\lambda = \dim E_\lambda. $$</span>
                                </p>
                                
                            </li>
                            </ul>
                            
                        </li><li>
                            <p>
                                The <em>defect</em> <span class="inline-math">$\Delta_\lambda$</span> is defined as
                                <span class="display-math">$$ \Delta_\lambda = M_\lambda - m_\lambda. $$</span>
                            </p>
                            
                        </li>
                        </ul>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed embed-result">
                    <div class="markdown-embed-title">
                        Proposition (non-negative defect)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-non-negative-defect.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            For any matrix <span class="inline-math">$A$</span> and its eigenvalue <span class="inline-math">$\lambda$</span>, the defect
                            <span class="display-math">$$ \Delta_\lambda \geq 0. $$</span>
                        </p>
                        <p>
                            <a href="https://math.stackexchange.com/questions/458189/why-geometric-multiplicity-is-bounded-by-algebraic-multiplicity?rq=1">Proof</a>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed embed-result">
                    <div class="markdown-embed-title">
                        Theorem (distinct eigenvalues imply linearly independent eigenvectors)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Theorem-distinct-eigenvalues-imply-linearly-independent-eigenvectors.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            Suppose an <span class="inline-math">$n\times n$</span> matrix <span class="inline-math">$A$</span> has <span class="inline-math">$n$</span> eigenvalues <span class="inline-math">$\lambda_1, \lambda_2, \ldots, \lambda_n$</span> corresponding to eigenvectors <span class="inline-math">$\v x_1, \v x_2, \ldots, \v x_n$</span>.
                        </p>
                        <p>
                            If <span class="inline-math">$\lambda_1, \ldots, \lambda_n$</span> are distinct, then <span class="inline-math">$\v x_1, \ldots, \v x_n$</span> are linearly independent.
                        </p>
                        <p>
                            <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-distinct-eigenvalues-imply-linearly-independent-eigenvectors.html">Proof</a>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Eigenvector basis
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Eigenvector-basis.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            By the <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Theorem-distinct-eigenvalues-imply-linearly-independent-eigenvectors.html">above theorem</a>, if <span class="inline-math">$n\times n$</span> matrix <span class="inline-math">$A$</span> has <span class="inline-math">$n$</span> distinct eigenvalues, then <span class="inline-math">$A$</span> has <span class="inline-math">$n$</span> linearly independent <span class="inline-math">$\v v_1, \v v_2, \ldots, \v v_n$</span>, so the matrix <span class="inline-math">$A$</span> <strong><em>with respect to this basis</em></strong> is diagonal:
                            <span class="display-math">$$ A\v v_i = \lambda_i\v v_i,\quad \forall i=1,2,\ldots,n $$</span>
                            <span class="display-math">$$ A = \pmat{
                            \lambda_1 &amp; 0 &amp; \ldots &amp; 0 \\
                            0 &amp; \lambda_2 &amp; \ldots &amp; 0 \\
                            \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                            0 &amp; 0 &amp; \ldots &amp; \lambda_n} $$</span>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Transformation matrices
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Transformation-matrices.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            Suppose we want to change the basis from <span class="inline-math">$\set{\v e_1, \v e_2, \ldots, \v e_n}$</span> to <span class="inline-math">$\set{\v e_1&#x27;, \v e_2&#x27;, \ldots, \v e_n&#x27;}$</span>.
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Change-of-basis matrix
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Change-of-basis-matrix.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Define the change-of-basis matrix <span class="inline-math">$P$</span> by (using summation notation)
                                        <span class="display-math">$$ \v e_j&#x27; = P_{ij} \v e_i. $$</span>
                                        To change it back, use the inverse matrix <span class="inline-math">$P\inv$</span>
                                        <span class="display-math">$$ \v e_i = (P\inv)_{ij} \v e_j&#x27;. $$</span>
                                        (<a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Derivation-of-change-of-basis-matrix.html">Derivation</a>)
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Transformation law for vectors
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Transformation-law-for-vectors.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        To transform a vector <span class="inline-math">$\v u$</span> from <span class="inline-math">$\v e_i$</span> to <span class="inline-math">$\v e_j&#x27;$</span> basis, multiply by <span class="inline-math">$P\inv$</span>:
                                        <span class="display-math">$$ u_j&#x27; = (P\inv)_{jk} u_k, \qquad \v u&#x27; = P\inv \v u. $$</span>
                                        To transform it back, multiply by <span class="inline-math">$P$</span>:
                                        <span class="display-math">$$ u_i = P_{ij} u_j&#x27;, \qquad \v u = P\v u&#x27;. $$</span>
                                        (<a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Derivation-of-transformation-law-for-vectors.html">Derivation and notes</a>)
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Example-of-transformation-law-for-vectors.html">Example</a>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Transformation law for matrices
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Transformation-law-for-matrices.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Consider linear map <span class="inline-math">$\alpha: \C^m \to \C^n$</span> with associated <span class="inline-math">$n\times m$</span> matrix <span class="inline-math">$A$</span>.
                                    </p>
                                    <p>
                                        To transform to a matrix that takes the input in the new basis and output a vector in the new basis, if <span class="inline-math">$P$</span> is the change-of-basis matrix of <span class="inline-math">$\C^m$</span> and <span class="inline-math">$Q$</span> is that of <span class="inline-math">$\C^n$</span>, use
                                        <span class="display-math">$$ A&#x27; = Q\inv A P. $$</span>
                                        (<a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Derivation-of-transformation-law-for-square-matrices.html">Derivation</a>)
                                    </p>
                                    <p>
                                        For <span class="inline-math">$\alpha: \C^n\to\C^n$</span>, we usually transform to the same basis in domain and codomain, so
                                        <span class="display-math">$$ A&#x27; = P\inv AP. $$</span>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-definition">
                                <div class="markdown-embed-title">
                                    Definition (similar matrix)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-similar-matrix.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Two <span class="inline-math">$n\times n$</span> matrices <span class="inline-math">$A,B$</span> are similar if there exists an invertible matrix <span class="inline-math">$P$</span> such that
                                        <span class="display-math">$$ B = P\inv A P. $$</span>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Proposition (similar matrices have the same characteristic polynomials)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-similar-matrices-have-the-same-characteristic-polynomials.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        If <span class="inline-math">$A,B$</span> are similar matrices, then their characteristic polynomials equal:
                                        <span class="display-math">$$ p_A(\lambda) = p_B(\lambda). $$</span>
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-similar-matrices-have-the-same-characteristic-polynomials.html">Proof</a>
                                    </p>
                                    <p>
                                        It <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Properties-of-the-characteristic-polynomial.html">follows that</a> <span class="inline-math">$\det A = \det B$</span> and <span class="inline-math">$\mrm{tr}\, A = \mrm{tr}\, B$</span>.
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Proposition (transformation matrix between orthonormal bases is unitary)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-transformation-matrix-between-orthonormal-bases-is-unitary.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Suppose <span class="inline-math">$U$</span> is the change-of-basis matrix between one <em><strong>orthonormal</strong></em> basis and a new <em><strong>orthonormal</strong></em> basis. Then <span class="inline-math">$U$</span> is unitary.
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-transformation-matrix-between-orthonormal-bases-is-unitary.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Diagonalisable matrices
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Diagonalisable-matrices.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            <div class="markdown-embed embed-definition">
                                <div class="markdown-embed-title">
                                    Definition (diagonalisable matrix)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-diagonalisable-matrix.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        An <span class="inline-math">$n\times n$</span> matrix <span class="inline-math">$A$</span> is diagonalisable if it is similar to a diagonal matrix.
                                    </p>
                                    <p>
                                        Equivalently, <span class="inline-math">$A$</span> is diagonalisable if its eigenvectors form a basis of <span class="inline-math">$\C^n$</span>.
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Theorem (basis vectors of eigenspaces are linearly independent)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Theorem-basis-vectors-of-eigenspaces-are-linearly-independent.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Let <span class="inline-math">$\lambda_1, \lambda_2, \ldots, \lambda_r$</span> with <span class="inline-math">$r\leq n$</span> be the distinct eigenvalues of an <span class="inline-math">$n\times n$</span> matrix <span class="inline-math">$A$</span>, and let <span class="inline-math">$B_i$</span> be the bases of the corresponding eigenspaces <span class="inline-math">$E_{\lambda_i}$</span>.
                                    </p>
                                    <p>
                                        Then the set <span class="inline-math">$B = \bigcup_{i=1}^r B_i$</span> is linearly independent.
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-basis-vectors-of-eigenspaces-are-linearly-independent.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Test for diagonalisability
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Test-for-diagonalisability.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <ol start="1">
                                    <li>
                                        <p>
                                            Solve <span class="inline-math">$p_A(\lambda) = 0$</span> for distinct solutions <span class="inline-math">$\lambda_1, \lambda_2, \ldots, \lambda_r$</span>;
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            Find the basis <span class="inline-math">$B_i$</span> of eigenspace <span class="inline-math">$E_{\lambda_i}$</span> to find <span class="inline-math">$m_{\lambda_i}$</span> for all <span class="inline-math">$i=1,2,\ldots, r$</span>;
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            Calculate the size of <span class="inline-math">$B$</span> <span class="inline-math">$= \sum_{i=1}^r m_{\lambda_i}$</span>.
                                        </p>
                                        <ul>
                                        <li>
                                            <p>
                                                If this equals <span class="inline-math">$n$</span>, then <span class="inline-math">$A$</span> is diagonalisable.
                                            </p>
                                            
                                        </li><li>
                                            <p>
                                                Otherwise <span class="inline-math">$A$</span> is not.
                                            </p>
                                            
                                        </li>
                                        </ul>
                                        
                                    </li>
                                    </ol>
                                    
                                </div>
                            </div>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Jordan normal form
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Jordan-normal-form.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Proposition (2x2 matrix has Jordan normal form)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-2x2-matrix-has-Jordan-normal-form.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Any <span class="inline-math">$2\times2$</span> complex matrix <span class="inline-math">$A$</span> is similar to one of
                                        <span class="display-math">$$ \pmat{\lambda_1&amp;0\\0&amp;\lambda_2}, \quad\pmat{\lambda&amp;0\\0&amp;\lambda}, \quad\pmat{\lambda&amp;1\\0&amp;\lambda}. $$</span>
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-2x2-matrix-has-Jordan-form.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Proposition (nxn matrix has Jordan normal form)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-nxn-matrix-has-Jordan-normal-form.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Every complex <span class="inline-math">$n\times n$</span> matrix is similar to a matrix <span class="inline-math">$A&#x27;$</span> satisfying
                                        <span class="display-math">$$ A&#x27;_{ij} = \begin{cases}
                                        \lambda_{i}, &amp;j=i, \\
                                        0 \text{ or } 1, &amp;j=i+1, \\
                                        0, &amp;\text{otherwise}.
                                        \end{cases} $$</span>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Example-finding-Jordan-normal-form.html">Example (finding Jordan normal form)</a>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Cayley-Hamilton theorem
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Cayley-Hamilton-theorem.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Theorem (Cayley-Hamilton theorem)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Theorem-Cayley-Hamilton-theorem.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Every <span class="inline-math">$n\times n$</span> complex matrix <span class="inline-math">$A$</span> satisfies its own characteristic equation, i.e.
                                        <span class="display-math">$$ p_A(A) = 0. $$</span>
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Partial-proof-Cayley-Hamilton-theorem.html">Partial proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Corollary (matrix inverse using coefficients in characteristic polynomial)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Corollary-matrix-inverse-using-coefficients-in-characteristic-polynomial.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        If <span class="inline-math">$A\inv$</span> exists and
                                        <span class="display-math">$$ p_A(x) = c_0 + c_1 x + \ldots + c_n x^n, $$</span>
                                        then
                                        <span class="display-math">$$ A\inv = -\frac1{c_0}(c_1 I + c_2 A + \ldots + c_n A^{n-1}). $$</span>
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-matrix-inverse-using-coefficients-in-characteristic-polynomial.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Eigenvectors of normal matrices
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Eigenvectors-of-normal-matrices.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Theorem (eigenvalues of Hermitian matrix is real)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Theorem-eigenvalues-of-Hermitian-matrix-is-real.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        The eigenvalues of a Hermitian matrix <span class="inline-math">$H$</span> are real.
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-eigenvalues-of-Hermitian-matrix-is-real.html">Proof</a>
                                    </p>
                                    <p>
                                        Consequently so are those of a real symmetric matrix.
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Lemma (distinct eigenvalues of normal matrix correspond to orthogonal eigenvectors)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Lemma-distinct-eigenvalues-of-normal-matrix-correspond-to-orthogonal-eigenvectors.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Eigenvectors of a normal matrix corresponding to distinct eigenvalues are orthogonal.
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-distinct-eigenvalues-of-normal-matrix-correspond-to-orthogonal-eigenvectors.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Theorem (normal matrix has orthogonal eigenvector basis)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Theorem-normal-matrix-has-orthogonal-eigenvector-basis.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        An <span class="inline-math">$n\times n$</span> normal matrix has <span class="inline-math">$n$</span> orthogonal eigenvectors.
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-normal-matrix-has-orthogonal-eigenvector-basis.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Theorem (normal matrix iff diagonalisable via unitary transform)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Theorem-normal-matrix-iff-diagonalisable-via-unitary-transform.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        A matrix <span class="inline-math">$A$</span> is similar to a diagonal matrix <span class="inline-math">$D$</span> via a unitary transform <span class="inline-math">$U$</span>, i.e.
                                        <span class="display-math">$$ A = UDU^\dagger, $$</span>
                                        iff <span class="inline-math">$A$</span> is normal.
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed embed-result">
                    <div class="markdown-embed-title">
                        Theorem (Gershgorin circle theorem)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Theorem-Gershgorin-circle-theorem.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            For an <span class="inline-math">$n\times n$</span> matrix <span class="inline-math">$A$</span>, for each row <span class="inline-math">$i$</span> define
                            <span class="display-math">$$ R_i = \sum_{j\neq i} |A_{ij}| $$</span>
                            and define a disk <span class="inline-math">$D_i(A_{ii}, R_i)\subset\C$</span> to be the circle with centre <span class="inline-math">$A_{ii}$</span> and radius <span class="inline-math">$R_i$</span>.
                        </p>
                        <p>
                            Then every eigenvalues lie within a disk <span class="inline-math">$D_i$</span> for some <span class="inline-math">$i$</span>.
                        </p>
                        <p>
                            <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-Gershgorin-circle-theorem.html">Proof</a>
                        </p>
                        
                    </div>
                </div>
            </p>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed">
        <div class="markdown-embed-title">
            Quadratic forms and conics
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Quadratic-forms-and-conics.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                <div class="markdown-embed embed-definition">
                    <div class="markdown-embed-title">
                        Definition (sesquilinear form, Hermitian form, quadratic form)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-sesquilinear-form-Hermitian-form-quadratic-form.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            For an <span class="inline-math">$n\times n$</span> matrix <span class="inline-math">$A$</span> and <span class="inline-math">$\v x\in\R^n$</span>, an expression of the form
                            <span class="display-math">$$\v x^T A \v x = x_i^* A_{ij} x_j$$</span>
                            is
                        </p>
                        <ul>
                        <li>
                            <p>
                                a <em>sesquilinear form</em>;
                            </p>
                            
                        </li><li>
                            <p>
                                a <em>Hermitian form</em> if <span class="inline-math">$A$</span> is Hermitian; and
                            </p>
                            
                        </li><li>
                            <p>
                                a <em>quadratic form</em> if <span class="inline-math">$A$</span> is real symmetric.
                            </p>
                            
                        </li>
                        </ul>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Properties of sesquilinear forms
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Properties-of-sesquilinear-forms.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <ul>
                        <li>
                            <p>
                                Hermitian forms are real. (<a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-Hermitian-forms-are-real.html">Proof</a>)
                            </p>
                            
                        </li><li>
                            <p>
                                Since any Hermitian matrix <span class="inline-math">$H$</span> can be diagonalised with a unitary transform <span class="inline-math">$U$</span>,
                                <span class="display-math">$$ F(\v x) = \v x^\dagger H\v x = \v x^\dagger UDU^\dagger\v x = \v x&#x27;^\dagger D \v x&#x27; $$</span>
                                where <span class="inline-math">$D = \mrm{diag}(\lambda_1, \lambda_2, \ldots, \lambda_n)$</span> and <span class="inline-math">$\v x&#x27; = U^\dagger\v x$</span> is the vector <span class="inline-math">$x$</span> relative to the orthonormal eigenvector basis called the <em>principal axes</em>.
                                Therefore, by inspecting
                                <span class="display-math">$$ F(\v x) = \pmat{x&#x27;_1 &amp; x&#x27;_2 &amp; \ldots &amp; x&#x27;_n}
                                \pmat{\lambda_1 &amp; 0 &amp; \ldots &amp; 0 \\ 0 &amp; \lambda_2 &amp; \ldots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \ldots &amp; \lambda_n}
                                \pmat{x_1&#x27; \\ x_2&#x27; \\ \vdots \\ x_n&#x27;},$$</span>
                                we can write
                                <span class="display-math">$$ F(\v x) = \sum_{i=1}^n \lambda_i(x_i)^2. $$</span>
                            </p>
                            
                        </li><li>
                            <p>
                                For any quadratic form <span class="inline-math">$\v x^T A \v x$</span>, we can write it as
                                <span class="display-math">$$ \v x^T A\v x = \v x^T S\v x, \quad \text{$S$ is real symmetric}. $$</span>
                                (<a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Derivation-quadratic-form-written-using-symmetric-matrix.html">Derivation</a>) Hence as above,
                                <span class="display-math">$$ \v x^T A\v x = \v x&#x27;^\dagger D\v x&#x27; $$</span>
                                where <span class="inline-math">$S = QDQ^T$</span>, <span class="inline-math">$\v x&#x27; = Q^T\v x$</span> and <span class="inline-math">$Q$</span> is a real diagonal matrix.
                            </p>
                            
                        </li>
                        </ul>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed embed-definition">
                    <div class="markdown-embed-title">
                        Definition (quadric)
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-quadric.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            A quadric is an <span class="inline-math">$n$</span>-dimensional surface defined by the zero of a real quadratic polynomial, i.e. its equation is
                            <span class="display-math">$$ \v x^T A\v x + \v b^T \v x + c = 0 $$</span>
                            where <span class="inline-math">$A$</span> is an <span class="inline-math">$n\times n$</span> real matrix, <span class="inline-math">$\v x$</span> and <span class="inline-math">$\v b$</span> are <span class="inline-math">$n$</span>-dimensional column vectors, and <span class="inline-math">$c$</span> is a constant.
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Equation of quadric equation
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Equation-of-quadric-equation.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <ul>
                        <li>
                            <p>
                                We have shown that <span class="inline-math">$\v x^T A \v x = \v x^T S\v x = \v x^T QDQ^T \v x$</span>. So by changing basis <span class="inline-math">$\v x \leftarrow Q^T\v x, \v b\leftarrow Q^T\v b$</span>, the quadric equation is equivalent to
                                <span class="display-math">$$ \v x^T D\v x + \v b^T \v x + c = 0 $$</span>
                                since <span class="inline-math">$(Q^T \v b)^T Q^T \v x = \v b^T \v x$</span> as <span class="inline-math">$QQ^T = I$</span>.
                            </p>
                            
                        </li><!-- BEGIN BLOCK ID fed174 -->
                        <li id="^fed174">
                            <p>
                                If <span class="inline-math">$S$</span> has no zero eigenvalue, then <span class="inline-math">$D$</span> is invertible. We can shift the origin to eliminate the linear term,
                                <span class="display-math">$$ \v x \leftarrow \v x + \frac 12 D\inv \v b. $$</span>
                                Then
                                <span class="display-math">$$ \v x^T D\v x = k. $$</span>
                                (<a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Derivation-elimination-of-linear-term-in-a-quadric-equation.html">Derivation</a>)
                            </p>
                            
                        </li>
                        <!-- END BLOCK ID fed174 -->
                        
                        </ul>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Quadric in 2D - conic sections
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Quadric-in-2D---conic-sections.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            Let <span class="inline-math">$D = \mrm{diag}(\lambda_1, \lambda_2)$</span>.
                        </p>
                        <ul>
                        <li>
                            <p>
                                If <span class="inline-math">$\lambda_1, \lambda_2\neq 0$</span>, we can <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Equation-of-quadric-equation.html#^fed174">eliminate</a> the linear term:
                                <span class="display-math">$$ \lambda_1 x_1^2 + \lambda_2 x_2^2 = k. $$</span>
                            </p>
                            <ul>
                            <li>
                                <p>
                                    If <span class="inline-math">$\lambda_1 \lambda_2 &gt; 0$</span>, then we required <span class="inline-math">$\mrm{sgn}(\lambda_1) = \mrm{sgn}(k).$</span> This is a ellipse with axes coinciding with eigenvectors of <span class="inline-math">$S$</span>, with semi-axes <span class="inline-math">$\sqrt{k/\lambda_1}$</span> and <span class="inline-math">$\sqrt{k/\lambda_2}$</span>.
                                </p>
                                <p>
                                    <img src="/tripos-notes/media/Lectures/Michaelmas/Vectors-and-Matrices-VM/Pasted-image-20231127220755.png" width="300.0">
                                </p>
                                
                            </li><li>
                                <p>
                                    If <span class="inline-math">$\lambda_1 \lambda_2 &lt; 0$</span>, set <span class="inline-math">$\lambda_1 = k/a^2 &gt; 0$</span> and <span class="inline-math">$\lambda_2 = -k/b^2 &lt; 0$</span>. This is an hyperbola
                                    <span class="display-math">$$ \frac{x_1^2}{a^2} - \frac{x_2^2}{b^2} = 1. $$</span>
                                </p>
                                <p>
                                    <img src="/tripos-notes/media/Lectures/Michaelmas/Vectors-and-Matrices-VM/Pasted-image-20231127220836.png" width="300.0">
                                </p>
                                
                            </li>
                            </ul>
                            
                        </li><li>
                            <p>
                                If <span class="inline-math">$\lambda_1 \lambda_2 = 0$</span>, take <span class="inline-math">$\lambda_2 = 0$</span>. <span class="inline-math">$\lambda_1\neq 0$</span> because otherwise <span class="inline-math">$A$</span> is just a zero matrix.
                                <span class="display-math">$$ \lambda_1 x_1^2 + b_1 x_1 + b_2 x_2 + c = 0. $$</span>
                            </p>
                            <ul>
                            <li>
                                <p>
                                    If <span class="inline-math">$b_2 \neq 0$</span>, then we can shift the origin <span class="inline-math">$x_1 \leftarrow x_1 - b_1/2\lambda_1$</span> to eliminate <span class="inline-math">$b_1x_1$</span>, and <span class="inline-math">$x_2 \leftarrow x_2 - c/b_2 + b_1^2/4\lambda_1 b_2$</span> to eliminate the constants to get a parabola
                                    <span class="display-math">$$ \lambda_1 + b_2 x_2 = 0. $$</span>
                                </p>
                                
                            </li><li>
                                <p>
                                    If <span class="inline-math">$b_2 = 0$</span>, then we get two straight line or no solution depending on the discriminant.
                                </p>
                                
                            </li>
                            </ul>
                            
                        </li>
                        </ul>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Focus-directrix property
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Focus-directrix-property.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            <div class="markdown-embed embed-definition">
                                <div class="markdown-embed-title">
                                    Definition (geometric definition of conic section)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-geometric-definition-of-conic-section.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        For a given eccentricity <span class="inline-math">$e$</span> and scale <span class="inline-math">$a$</span>, define
                                    </p>
                                    <ul>
                                    <li>
                                        <p>
                                            foci be <span class="inline-math">$(\pm ae, 0)$</span> and
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            directrix <span class="inline-math">$x = \pm a/e$</span>.
                                        </p>
                                        
                                    </li>
                                    </ul>
                                    <p>
                                        A conic section is the set of points satisfying the property that
                                        <span class="display-math">$$ \text{distance from focus} = e\times\text{distance from closer directrix}$$</span>
                                        except when <span class="inline-math">$e=1$</span>, then take the distance to the further directrix.
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Eccentricity and type of conic section
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Eccentricity-and-type-of-conic-section.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <table>
                                        <tr>
                                            <th class="table-cell-left"><p>
                                                <span class="inline-math">$e$</span>
                                            </p>
                                            </th>
                                            <th class="table-cell-left"><p>
                                                Type of conic section
                                            </p>
                                            </th>
                                        </tr>
                                        <tr>
                                            <td class="table-cell-left"><p>
                                                <span class="inline-math">$0$</span>
                                            </p>
                                            </td>
                                            <td class="table-cell-left"><p>
                                                Circle
                                            </p>
                                            </td>
                                        </tr>
                                        <tr>
                                            <td class="table-cell-left"><p>
                                                <span class="inline-math">$(0,1)$</span>
                                            </p>
                                            </td>
                                            <td class="table-cell-left"><p>
                                                Ellipse
                                            </p>
                                            </td>
                                        </tr>
                                        <tr>
                                            <td class="table-cell-left"><p>
                                                <span class="inline-math">$1$</span>
                                            </p>
                                            </td>
                                            <td class="table-cell-left"><p>
                                                Parabola
                                            </p>
                                            </td>
                                        </tr>
                                        <tr>
                                            <td class="table-cell-left"><p>
                                                <span class="inline-math">$(1, \infty)$</span>
                                            </p>
                                            </td>
                                            <td class="table-cell-left"><p>
                                                Hyperbola
                                            </p>
                                            </td>
                                        </tr>
                                    </table>
                                    <p>
                                        <p>
                                    </p>
                                    <p>
                                        <iframe src="https://www.desmos.com/calculator/0ugbajvnn0?embed" width="100%" height="100"></iframe>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Polar focus-directrix property
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Polar-focus-directrix-property.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Let the distance between a focus and the closer directrix be <span class="inline-math">$l/e$</span>, so <span class="inline-math">$l = a|1-e^2|$</span>.
                                    </p>
                                    <p>
                                        <img src="/tripos-notes/media/Lectures/Michaelmas/Vectors-and-Matrices-VM/Pasted-image-20231127223758.png" width="400.0">
                                    </p>
                                    <p>
                                        So putting the origin at one of the focus,
                                        <span class="display-math">$$\begin{align*}
                                        r &amp;= e\big(\frac le - r\cos \theta\big) \\
                                        r &amp;= \frac{l}{1 + r\cos\theta}.
                                        \end{align*}$$</span>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        
                    </div>
                </div>
            </p>
            
        </div>
    </div>
</p>
<p>
    <div class="markdown-embed">
        <div class="markdown-embed-title">
            Transformation group
            <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Transformation-group.html">open_in_new</a>
        </div>
        <div class="markdown-embed-content">
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Orthogonal groups
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Groups-Gp/Orthogonal-groups.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            <div class="markdown-embed embed-definition">
                                <div class="markdown-embed-title">
                                    Definition (orthogonal groups)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Groups-Gp/Definition-orthogonal-groups.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        The <em><span class="inline-math">$n$</span>-dimensional orthogonal group</em> is the subgroup of <span class="inline-math">$\gl n\R$</span> that preserves distance in <span class="inline-math">$\R^n$</span>:
                                        <span class="display-math">$$ O(n) = \sb{A\in\gl n\R}{\|Av\| = \|v\|\ \forall v\in\R^n}. $$</span>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Lemma (equivalent definitions of orthogonal matrices)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Groups-Gp/Lemma-equivalent-definitions-of-orthogonal-matrices.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Let <span class="inline-math">$P\in M_n(\R)$</span>. The following are equivalent:
                                    </p>
                                    <ol start="1">
                                    <li>
                                        <p>
                                            <span class="inline-math">$P^T P = I$</span>.
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            <span class="inline-math">$|P\v x| = |\v x|$</span> for all <span class="inline-math">$\v x\in\R^n$</span>;
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            <span class="inline-math">$(P\v x)\cdot(P\v y) = \v x\cdot\v y$</span> for all <span class="inline-math">$\v x,\v y\in\R^n$</span>.
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            if <span class="inline-math">$(\v v_1, \v v_2, \ldots, \v v_n)$</span> are orthonormal, then so are <span class="inline-math">$(P\v v_1, P\v v_2, \ldots, P\v v_n)$</span>;
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            columns of <span class="inline-math">$P$</span> form an orthonormal basis.
                                        </p>
                                        
                                    </li>
                                    </ol>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Groups-Gp/Proof-equivalent-definitions-of-orthogonal-matrices.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Proposition (determinant of orthogonal and unitary matrix)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-determinant-of-orthogonal-and-unitary-matrix.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <ul>
                                    <li>
                                        <p>
                                            If <span class="inline-math">$A$</span> is orthogonal, then <span class="inline-math">$\det A = \pm 1$</span> (<a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-determinant-of-orthogonal-matrix.html">Proof</a>). In particular, in <span class="inline-math">$\R^3$</span>, <span class="inline-math">$A$</span> represents a rotation (<span class="inline-math">$\det A = 1$</span>) or a reflection (<span class="inline-math">$\det A = -1$</span>).
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            If <span class="inline-math">$U$</span> is unitary, then <span class="inline-math">$|\det U| = 1$</span> (<a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-determinant-of-unitary-matrix.html">Proof</a>).
                                        </p>
                                        
                                    </li>
                                    </ul>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-definition">
                                <div class="markdown-embed-title">
                                    Definition (special orthogonal group)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Groups-Gp/Definition-special-orthogonal-group.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        The special orthogonal group is the group of all orthogonal matrices with determinant 1,
                                        <span class="display-math">$$ \mrm{SO}(n) = O(n) \inter \sl n\R = \sb{A\in O(n)}{\det A=1}.$$</span>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-definition">
                                <div class="markdown-embed-title">
                                    Definition (reflection)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Groups-Gp/Definition-reflection.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Any <span class="inline-math">$v\in\R^n \setminus \set 0$</span> define an orthogonal hyperplane
                                        <span class="inline-math">$P_v = \sb{x\in R^n}{x\cdot v = 0}.$</span>
                                        The reflection in <span class="inline-math">$P_v$</span> is defined to be
                                        <span class="display-math">$$ S_v(x) = x - \frac{2(x\cdot v)}{\|v\|^2}v. $$</span>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed">
                                <div class="markdown-embed-title">
                                    Properties of reflection
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Groups-Gp/Properties-of-reflection.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <ul>
                                    <li>
                                        <p>
                                            We can assume <span class="inline-math">$v$</span> is normalised to <span class="inline-math">$v/\|v\|$</span>.
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            <span class="inline-math">$S_v^2 = I$</span>. (<a href="/tripos-notes/Lectures/Michaelmas/Groups-Gp/Proof-reflection-squared-is-identity.html">Proof</a>)
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            <span class="inline-math">$S_v \in O(n)$</span>. (<a href="/tripos-notes/Lectures/Michaelmas/Groups-Gp/Proof-reflection-is-orthogonal.html">Proof</a>)
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            <span class="inline-math">$\det S_v = -1$</span>, so <span class="inline-math">$S_v\not\in \mrm{SO}(n)$</span>. (<a href="/tripos-notes/Lectures/Michaelmas/Groups-Gp/Proof-determinant-of-reflection-is--1.html">Proof</a>)
                                        </p>
                                        
                                    </li>
                                    </ul>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Theorem (reflection generate $O(n)$)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Groups-Gp/Theorem-reflection-generate-O-n.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Every <span class="inline-math">$A\in O(n)$</span> is a product of at most <span class="inline-math">$n$</span> reflections.
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Groups-Gp/Proof-reflection-generate-O-n.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Lemma (elements of $O(2)$)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Groups-Gp/Lemma-elements-of-O-2.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Let <span class="inline-math">$A\in O(2)$</span>.
                                    </p>
                                    <ul>
                                    <li>
                                        <p>
                                            If <span class="inline-math">$A\not\in \mrm{SO}(2)$</span>, then <span class="inline-math">$A$</span> is a reflection;
                                        </p>
                                        
                                    </li><li>
                                        <p>
                                            If <span class="inline-math">$A\in\mrm{SO}(2)$</span>, then <span class="inline-math">$A$</span> is a rotation, here means that <span class="inline-math">$A=I$</span> or <span class="inline-math">$\mrm{Fix}(A) = \set 0$</span>.
                                        </p>
                                        
                                    </li>
                                    </ul>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Groups-Gp/Proof-elements-of-O-2.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Lemma ($SO(3)$ fixes a line)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Groups-Gp/Lemma-SO-3-fixes-a-line.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        If <span class="inline-math">$A\in\mrm{SO}(3)$</span>, then <span class="inline-math">$A$</span> is a rotation, here defined as <span class="inline-math">$A=I$</span> or <span class="inline-math">$\mrm{Fix}(A)$</span> is a line.
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Groups-Gp/Proof-SO-3-fixes-a-line.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        
                    </div>
                </div>
            </p>
            <p>
                <div class="markdown-embed">
                    <div class="markdown-embed-title">
                        Minkowski spacetime
                        <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Minkowski-spacetime.html">open_in_new</a>
                    </div>
                    <div class="markdown-embed-content">
                        <p>
                            Consider a Minkowski 1+1 dimensional spacetime, i.e. one dimension of space and one dimension of time.
                        </p>
                        <p>
                            <div class="markdown-embed embed-definition">
                                <div class="markdown-embed-title">
                                    Definition (Minkowski inner product)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-Minkowski-inner-product.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Define the Minkowski inner product of <span class="inline-math">$\v x,\v y$</span> to be
                                        <span class="display-math">$$ \langle \v x|\v y\rangle \equiv \v x^T \Lambda \v x \equiv x_1 y_1 - x_2 y_2, \quad \Lambda = \pmat{1 &amp; 0 \\ 0 &amp; -1}. $$</span>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Proposition (Minkowski inner product-preserving transformation)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-Minkowski-inner-product-preserving-transformation.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        If a matrix <span class="inline-math">$M$</span> has top right entry <span class="inline-math">$M_{11} &gt; 0$</span> (to preserve forward/backward light cone) and preserves the Minkowski inner product, i.e.
                                        <span class="display-math">$$ \langle \v x|\v y\rangle = \langle M\v x|M\v y\rangle, $$</span>
                                        then <span class="inline-math">$M$</span> satisfies
                                        <span class="display-math">$$ \Lambda = M^T \Lambda M $$</span>
                                        and is of the form <span class="inline-math">$H_\alpha$</span> or <span class="inline-math">$K_{\alpha/2}$</span>, where
                                        <span class="display-math">$$ H_\alpha = \pmat{\cosh\alpha &amp; \sinh\alpha \\ \sinh\alpha &amp; \cosh\alpha}, \quad K_{\alpha/2} = \pmat{\cosh\alpha &amp; -\sinh\alpha \\ \sinh\alpha &amp; -\cosh\alpha}. $$</span>
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-Minkowski-inner-product-preserving-transformation.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-definition">
                                <div class="markdown-embed-title">
                                    Definition (Lorentz matrix)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Definition-Lorentz-matrix.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        Define the Lorentz matrix as
                                        <span class="display-math">$$ B_v \equiv \frac1{\sqrt{1-v^2}}\pmat{1 &amp; v \\ v &amp; 1} = H_{\tanh\inv v} $$</span>
                                        for some velocity <span class="inline-math">$v$</span> in natural unit, i.e. speed of light = 1.
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        <p>
                            <div class="markdown-embed embed-result">
                                <div class="markdown-embed-title">
                                    Proposition (Lorentz matrix forms a group)
                                    <a class="markdown-embed-open" title="Open" href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proposition-Lorentz-matrix-forms-a-group.html">open_in_new</a>
                                </div>
                                <div class="markdown-embed-content">
                                    <p>
                                        The set of all Lorentz matrix <span class="inline-math">$\sb{B_v}{v \in (-1,1)}$</span> forms a group.
                                    </p>
                                    <p>
                                        <a href="/tripos-notes/Lectures/Michaelmas/Vectors-and-Matrices-VM/Proof-Lorentz-matrix-forms-a-group.html">Proof</a>
                                    </p>
                                    
                                </div>
                            </div>
                        </p>
                        
                    </div>
                </div>
            </p>
            
        </div>
    </div>
</p>
